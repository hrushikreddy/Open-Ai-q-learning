{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Hyper Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDAkLcs-cPfA",
        "outputId": "2a56c0ca-9fb6-48fd-dec3-c96b7fb5db59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 3,626 B/3,626 B 100\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Conn\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [5 InRelease 14.2 kB/88.7 kB 16%] [3 InRelease 14\u001b[0m\r                                                                               \rGet:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,496 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,728 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [907 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,165 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [941 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,950 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [999 kB]\n",
            "Get:24 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 14.8 MB in 5s (3,276 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "56 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libpq-dev is already the newest version (10.19-0ubuntu0.18.04.1).\n",
            "libpq-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  gir1.2-ibus-1.0 libcapnp-0.6.1 libdbus-1-dev libdmx-dev libdmx1\n",
            "  libfontenc-dev libfs-dev libfs6 libibus-1.0-5 libibus-1.0-dev\n",
            "  libmirclient-dev libmirclient9 libmircommon-dev libmircommon7\n",
            "  libmircookie-dev libmircookie2 libmircore-dev libmircore1 libmirprotobuf3\n",
            "  libpciaccess-dev libpixman-1-dev libprotobuf-dev libprotobuf-lite10\n",
            "  libpulse-dev libpulse-mainloop-glib0 libsndio-dev libudev-dev libxaw7-dev\n",
            "  libxcomposite-dev libxcursor-dev libxfont-dev libxinerama-dev\n",
            "  libxkbcommon-dev libxkbfile-dev libxmuu-dev libxpm-dev libxrandr-dev\n",
            "  libxres-dev libxres1 libxtst-dev libxv-dev libxvmc-dev libxvmc1\n",
            "  libxxf86dga-dev libxxf86dga1 mir-client-platform-mesa-dev swig3.0\n",
            "  x11proto-composite-dev x11proto-dri2-dev x11proto-fonts-dev x11proto-gl-dev\n",
            "  x11proto-randr-dev x11proto-record-dev x11proto-render-dev\n",
            "  x11proto-resource-dev x11proto-xf86bigfont-dev x11proto-xf86dga-dev\n",
            "  x11proto-xinerama-dev xserver-xorg-dev\n",
            "Suggested packages:\n",
            "  libxaw-doc swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  gir1.2-ibus-1.0 libcapnp-0.6.1 libdbus-1-dev libdmx-dev libdmx1\n",
            "  libfontenc-dev libfs-dev libfs6 libibus-1.0-5 libibus-1.0-dev\n",
            "  libmirclient-dev libmirclient9 libmircommon-dev libmircommon7\n",
            "  libmircookie-dev libmircookie2 libmircore-dev libmircore1 libmirprotobuf3\n",
            "  libpciaccess-dev libpixman-1-dev libprotobuf-dev libprotobuf-lite10\n",
            "  libpulse-dev libpulse-mainloop-glib0 libsdl2-dev libsndio-dev libudev-dev\n",
            "  libxaw7-dev libxcomposite-dev libxcursor-dev libxfont-dev libxinerama-dev\n",
            "  libxkbcommon-dev libxkbfile-dev libxmuu-dev libxpm-dev libxrandr-dev\n",
            "  libxres-dev libxres1 libxtst-dev libxv-dev libxvmc-dev libxvmc1\n",
            "  libxxf86dga-dev libxxf86dga1 mir-client-platform-mesa-dev swig swig3.0\n",
            "  x11proto-composite-dev x11proto-dri2-dev x11proto-fonts-dev x11proto-gl-dev\n",
            "  x11proto-randr-dev x11proto-record-dev x11proto-render-dev\n",
            "  x11proto-resource-dev x11proto-xf86bigfont-dev x11proto-xf86dga-dev\n",
            "  x11proto-xinerama-dev xorg-dev xserver-xorg-dev xvfb\n",
            "0 upgraded, 63 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 6,943 kB of archives.\n",
            "After this operation, 38.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdmx1 amd64 1:1.1.3-1 [10.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibus-1.0-5 amd64 1.5.17-3ubuntu5.3 [133 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gir1.2-ibus-1.0 amd64 1.5.17-3ubuntu5.3 [66.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcapnp-0.6.1 amd64 0.6.1-1ubuntu1 [658 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdbus-1-dev amd64 1.12.2-1ubuntu1.2 [165 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdmx-dev amd64 1:1.1.3-1 [34.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfontenc-dev amd64 1:1.1.3-1 [13.8 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfs6 amd64 2:1.0.7-1 [22.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-fonts-dev all 2018.4-4 [2,620 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfs-dev amd64 2:1.0.7-1 [26.8 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibus-1.0-dev amd64 1.5.17-3ubuntu5.3 [145 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore1 amd64 0.31.1-0ubuntu1 [26.5 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon7 amd64 0.31.1-0ubuntu1 [73.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-lite10 amd64 3.0.0-9.1ubuntu1 [97.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirprotobuf3 amd64 0.31.1-0ubuntu1 [127 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient9 amd64 0.31.1-0ubuntu1 [199 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore-dev amd64 0.31.1-0ubuntu1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-dev amd64 3.0.0-9.1ubuntu1 [959 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxkbcommon-dev amd64 0.8.2-1~ubuntu18.04.1 [150 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon-dev amd64 0.31.1-0ubuntu1 [13.9 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie2 amd64 0.31.1-0ubuntu1 [19.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie-dev amd64 0.31.1-0ubuntu1 [4,392 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient-dev amd64 0.31.1-0ubuntu1 [47.8 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-dev amd64 0.34.0-2 [244 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-mainloop-glib0 amd64 1:11.1-1ubuntu7.11 [22.1 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-dev amd64 1:11.1-1ubuntu7.11 [81.5 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsndio-dev amd64 1.1.0-3 [13.3 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libudev-dev amd64 237-3ubuntu10.53 [19.1 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcursor-dev amd64 1:1.1.15-1 [26.5 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xinerama-dev all 2018.4-4 [2,628 B]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxinerama-dev amd64 2:1.1.3-1 [8,404 B]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-randr-dev all 2018.4-4 [2,620 B]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrandr-dev amd64 2:1.5.1-1 [24.0 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxv-dev amd64 2:1.0.11-1 [32.5 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsdl2-dev amd64 2.0.8+dfsg1-1ubuntu1.18.04.4 [683 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxpm-dev amd64 1:3.5.12-1 [87.4 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxaw7-dev amd64 2:1.0.13-1 [231 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-composite-dev all 1:2018.4-4 [2,620 B]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcomposite-dev amd64 1:0.4.4-2 [9,136 B]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxfont-dev amd64 1:2.0.3-1 [118 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxkbfile-dev amd64 1:1.0.9-2 [74.3 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxmuu-dev amd64 2:1.1.2-2 [7,056 B]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxres1 amd64 2:1.2.0-2 [7,716 B]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-resource-dev all 2018.4-4 [2,620 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxres-dev amd64 2:1.2.0-2 [8,136 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-record-dev all 2018.4-4 [2,620 B]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxtst-dev amd64 2:1.2.3-1 [15.2 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxvmc1 amd64 2:1.0.10-1 [13.7 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxvmc-dev amd64 2:1.0.10-1 [21.3 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xf86dga-dev all 2018.4-4 [2,624 B]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga-dev amd64 2:1.1.4-1 [17.6 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic/main amd64 mir-client-platform-mesa-dev amd64 0.31.1-0ubuntu1 [11.0 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-dri2-dev all 2018.4-4 [2,620 B]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-gl-dev all 2018.4-4 [2,612 B]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-render-dev all 2:2018.4-4 [2,620 B]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xf86bigfont-dev all 2018.4-4 [2,628 B]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpciaccess-dev amd64 0.14-1 [20.2 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xserver-xorg-dev amd64 2:1.19.6-1ubuntu4.10 [198 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xorg-dev all 1:7.7+19ubuntu7.1 [4,300 B]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 6,943 kB in 1s (9,468 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libdmx1:amd64.\n",
            "(Reading database ... 155501 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libdmx1_1%3a1.1.3-1_amd64.deb ...\n",
            "Unpacking libdmx1:amd64 (1:1.1.3-1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../01-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package libibus-1.0-5:amd64.\n",
            "Preparing to unpack .../02-libibus-1.0-5_1.5.17-3ubuntu5.3_amd64.deb ...\n",
            "Unpacking libibus-1.0-5:amd64 (1.5.17-3ubuntu5.3) ...\n",
            "Selecting previously unselected package gir1.2-ibus-1.0:amd64.\n",
            "Preparing to unpack .../03-gir1.2-ibus-1.0_1.5.17-3ubuntu5.3_amd64.deb ...\n",
            "Unpacking gir1.2-ibus-1.0:amd64 (1.5.17-3ubuntu5.3) ...\n",
            "Selecting previously unselected package libcapnp-0.6.1:amd64.\n",
            "Preparing to unpack .../04-libcapnp-0.6.1_0.6.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcapnp-0.6.1:amd64 (0.6.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\n",
            "Preparing to unpack .../05-libdbus-1-dev_1.12.2-1ubuntu1.2_amd64.deb ...\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.2-1ubuntu1.2) ...\n",
            "Selecting previously unselected package libdmx-dev:amd64.\n",
            "Preparing to unpack .../06-libdmx-dev_1%3a1.1.3-1_amd64.deb ...\n",
            "Unpacking libdmx-dev:amd64 (1:1.1.3-1) ...\n",
            "Selecting previously unselected package libfontenc-dev:amd64.\n",
            "Preparing to unpack .../07-libfontenc-dev_1%3a1.1.3-1_amd64.deb ...\n",
            "Unpacking libfontenc-dev:amd64 (1:1.1.3-1) ...\n",
            "Selecting previously unselected package libfs6:amd64.\n",
            "Preparing to unpack .../08-libfs6_2%3a1.0.7-1_amd64.deb ...\n",
            "Unpacking libfs6:amd64 (2:1.0.7-1) ...\n",
            "Selecting previously unselected package x11proto-fonts-dev.\n",
            "Preparing to unpack .../09-x11proto-fonts-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-fonts-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libfs-dev:amd64.\n",
            "Preparing to unpack .../10-libfs-dev_2%3a1.0.7-1_amd64.deb ...\n",
            "Unpacking libfs-dev:amd64 (2:1.0.7-1) ...\n",
            "Selecting previously unselected package libibus-1.0-dev:amd64.\n",
            "Preparing to unpack .../11-libibus-1.0-dev_1.5.17-3ubuntu5.3_amd64.deb ...\n",
            "Unpacking libibus-1.0-dev:amd64 (1.5.17-3ubuntu5.3) ...\n",
            "Selecting previously unselected package libmircore1:amd64.\n",
            "Preparing to unpack .../12-libmircore1_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircore1:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmircommon7:amd64.\n",
            "Preparing to unpack .../13-libmircommon7_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircommon7:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libprotobuf-lite10:amd64.\n",
            "Preparing to unpack .../14-libprotobuf-lite10_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package libmirprotobuf3:amd64.\n",
            "Preparing to unpack .../15-libmirprotobuf3_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmirprotobuf3:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmirclient9:amd64.\n",
            "Preparing to unpack .../16-libmirclient9_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmirclient9:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmircore-dev:amd64.\n",
            "Preparing to unpack .../17-libmircore-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircore-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libprotobuf-dev:amd64.\n",
            "Preparing to unpack .../18-libprotobuf-dev_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package libxkbcommon-dev:amd64.\n",
            "Preparing to unpack .../19-libxkbcommon-dev_0.8.2-1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libxkbcommon-dev:amd64 (0.8.2-1~ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libmircommon-dev:amd64.\n",
            "Preparing to unpack .../20-libmircommon-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircommon-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmircookie2:amd64.\n",
            "Preparing to unpack .../21-libmircookie2_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircookie2:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmircookie-dev:amd64.\n",
            "Preparing to unpack .../22-libmircookie-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircookie-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmirclient-dev:amd64.\n",
            "Preparing to unpack .../23-libmirclient-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmirclient-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libpixman-1-dev:amd64.\n",
            "Preparing to unpack .../24-libpixman-1-dev_0.34.0-2_amd64.deb ...\n",
            "Unpacking libpixman-1-dev:amd64 (0.34.0-2) ...\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\n",
            "Preparing to unpack .../25-libpulse-mainloop-glib0_1%3a11.1-1ubuntu7.11_amd64.deb ...\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.11) ...\n",
            "Selecting previously unselected package libpulse-dev:amd64.\n",
            "Preparing to unpack .../26-libpulse-dev_1%3a11.1-1ubuntu7.11_amd64.deb ...\n",
            "Unpacking libpulse-dev:amd64 (1:11.1-1ubuntu7.11) ...\n",
            "Selecting previously unselected package libsndio-dev:amd64.\n",
            "Preparing to unpack .../27-libsndio-dev_1.1.0-3_amd64.deb ...\n",
            "Unpacking libsndio-dev:amd64 (1.1.0-3) ...\n",
            "Selecting previously unselected package libudev-dev:amd64.\n",
            "Preparing to unpack .../28-libudev-dev_237-3ubuntu10.53_amd64.deb ...\n",
            "Unpacking libudev-dev:amd64 (237-3ubuntu10.53) ...\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\n",
            "Preparing to unpack .../29-libxcursor-dev_1%3a1.1.15-1_amd64.deb ...\n",
            "Unpacking libxcursor-dev:amd64 (1:1.1.15-1) ...\n",
            "Selecting previously unselected package x11proto-xinerama-dev.\n",
            "Preparing to unpack .../30-x11proto-xinerama-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-xinerama-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\n",
            "Preparing to unpack .../31-libxinerama-dev_2%3a1.1.3-1_amd64.deb ...\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.3-1) ...\n",
            "Selecting previously unselected package x11proto-randr-dev.\n",
            "Preparing to unpack .../32-x11proto-randr-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-randr-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\n",
            "Preparing to unpack .../33-libxrandr-dev_2%3a1.5.1-1_amd64.deb ...\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.1-1) ...\n",
            "Selecting previously unselected package libxv-dev:amd64.\n",
            "Preparing to unpack .../34-libxv-dev_2%3a1.0.11-1_amd64.deb ...\n",
            "Unpacking libxv-dev:amd64 (2:1.0.11-1) ...\n",
            "Selecting previously unselected package libsdl2-dev:amd64.\n",
            "Preparing to unpack .../35-libsdl2-dev_2.0.8+dfsg1-1ubuntu1.18.04.4_amd64.deb ...\n",
            "Unpacking libsdl2-dev:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.4) ...\n",
            "Selecting previously unselected package libxpm-dev:amd64.\n",
            "Preparing to unpack .../36-libxpm-dev_1%3a3.5.12-1_amd64.deb ...\n",
            "Unpacking libxpm-dev:amd64 (1:3.5.12-1) ...\n",
            "Selecting previously unselected package libxaw7-dev:amd64.\n",
            "Preparing to unpack .../37-libxaw7-dev_2%3a1.0.13-1_amd64.deb ...\n",
            "Unpacking libxaw7-dev:amd64 (2:1.0.13-1) ...\n",
            "Selecting previously unselected package x11proto-composite-dev.\n",
            "Preparing to unpack .../38-x11proto-composite-dev_1%3a2018.4-4_all.deb ...\n",
            "Unpacking x11proto-composite-dev (1:2018.4-4) ...\n",
            "Selecting previously unselected package libxcomposite-dev:amd64.\n",
            "Preparing to unpack .../39-libxcomposite-dev_1%3a0.4.4-2_amd64.deb ...\n",
            "Unpacking libxcomposite-dev:amd64 (1:0.4.4-2) ...\n",
            "Selecting previously unselected package libxfont-dev.\n",
            "Preparing to unpack .../40-libxfont-dev_1%3a2.0.3-1_amd64.deb ...\n",
            "Unpacking libxfont-dev (1:2.0.3-1) ...\n",
            "Selecting previously unselected package libxkbfile-dev:amd64.\n",
            "Preparing to unpack .../41-libxkbfile-dev_1%3a1.0.9-2_amd64.deb ...\n",
            "Unpacking libxkbfile-dev:amd64 (1:1.0.9-2) ...\n",
            "Selecting previously unselected package libxmuu-dev:amd64.\n",
            "Preparing to unpack .../42-libxmuu-dev_2%3a1.1.2-2_amd64.deb ...\n",
            "Unpacking libxmuu-dev:amd64 (2:1.1.2-2) ...\n",
            "Selecting previously unselected package libxres1:amd64.\n",
            "Preparing to unpack .../43-libxres1_2%3a1.2.0-2_amd64.deb ...\n",
            "Unpacking libxres1:amd64 (2:1.2.0-2) ...\n",
            "Selecting previously unselected package x11proto-resource-dev.\n",
            "Preparing to unpack .../44-x11proto-resource-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-resource-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxres-dev.\n",
            "Preparing to unpack .../45-libxres-dev_2%3a1.2.0-2_amd64.deb ...\n",
            "Unpacking libxres-dev (2:1.2.0-2) ...\n",
            "Selecting previously unselected package x11proto-record-dev.\n",
            "Preparing to unpack .../46-x11proto-record-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-record-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxtst-dev:amd64.\n",
            "Preparing to unpack .../47-libxtst-dev_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst-dev:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package libxvmc1:amd64.\n",
            "Preparing to unpack .../48-libxvmc1_2%3a1.0.10-1_amd64.deb ...\n",
            "Unpacking libxvmc1:amd64 (2:1.0.10-1) ...\n",
            "Selecting previously unselected package libxvmc-dev:amd64.\n",
            "Preparing to unpack .../49-libxvmc-dev_2%3a1.0.10-1_amd64.deb ...\n",
            "Unpacking libxvmc-dev:amd64 (2:1.0.10-1) ...\n",
            "Selecting previously unselected package x11proto-xf86dga-dev.\n",
            "Preparing to unpack .../50-x11proto-xf86dga-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-xf86dga-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxxf86dga-dev:amd64.\n",
            "Preparing to unpack .../51-libxxf86dga-dev_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga-dev:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package mir-client-platform-mesa-dev:amd64.\n",
            "Preparing to unpack .../52-mir-client-platform-mesa-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking mir-client-platform-mesa-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../53-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../54-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package x11proto-dri2-dev.\n",
            "Preparing to unpack .../55-x11proto-dri2-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-dri2-dev (2018.4-4) ...\n",
            "Selecting previously unselected package x11proto-gl-dev.\n",
            "Preparing to unpack .../56-x11proto-gl-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-gl-dev (2018.4-4) ...\n",
            "Selecting previously unselected package x11proto-render-dev.\n",
            "Preparing to unpack .../57-x11proto-render-dev_2%3a2018.4-4_all.deb ...\n",
            "Unpacking x11proto-render-dev (2:2018.4-4) ...\n",
            "Selecting previously unselected package x11proto-xf86bigfont-dev.\n",
            "Preparing to unpack .../58-x11proto-xf86bigfont-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-xf86bigfont-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libpciaccess-dev:amd64.\n",
            "Preparing to unpack .../59-libpciaccess-dev_0.14-1_amd64.deb ...\n",
            "Unpacking libpciaccess-dev:amd64 (0.14-1) ...\n",
            "Selecting previously unselected package xserver-xorg-dev.\n",
            "Preparing to unpack .../60-xserver-xorg-dev_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xserver-xorg-dev (2:1.19.6-1ubuntu4.10) ...\n",
            "Selecting previously unselected package xorg-dev.\n",
            "Preparing to unpack .../61-xorg-dev_1%3a7.7+19ubuntu7.1_all.deb ...\n",
            "Unpacking xorg-dev (1:7.7+19ubuntu7.1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../62-xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up x11proto-fonts-dev (2018.4-4) ...\n",
            "Setting up x11proto-dri2-dev (2018.4-4) ...\n",
            "Setting up libdbus-1-dev:amd64 (1.12.2-1ubuntu1.2) ...\n",
            "Setting up libxcursor-dev:amd64 (1:1.1.15-1) ...\n",
            "Setting up libxkbcommon-dev:amd64 (0.8.2-1~ubuntu18.04.1) ...\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.11) ...\n",
            "Setting up libpulse-dev:amd64 (1:11.1-1ubuntu7.11) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up libpciaccess-dev:amd64 (0.14-1) ...\n",
            "Setting up libmircore-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libxres1:amd64 (2:1.2.0-2) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up libxpm-dev:amd64 (1:3.5.12-1) ...\n",
            "Setting up libsndio-dev:amd64 (1.1.0-3) ...\n",
            "Setting up libxres-dev (2:1.2.0-2) ...\n",
            "Setting up x11proto-xf86bigfont-dev (2018.4-4) ...\n",
            "Setting up libxkbfile-dev:amd64 (1:1.0.9-2) ...\n",
            "Setting up libxvmc1:amd64 (2:1.0.10-1) ...\n",
            "Setting up libxmuu-dev:amd64 (2:1.1.2-2) ...\n",
            "Setting up x11proto-record-dev (2018.4-4) ...\n",
            "Setting up libxtst-dev:amd64 (2:1.2.3-1) ...\n",
            "Setting up libmircookie2:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libfontenc-dev:amd64 (1:1.1.3-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11proto-xinerama-dev (2018.4-4) ...\n",
            "Setting up libpixman-1-dev:amd64 (0.34.0-2) ...\n",
            "Setting up x11proto-randr-dev (2018.4-4) ...\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.3-1) ...\n",
            "Setting up x11proto-gl-dev (2018.4-4) ...\n",
            "Setting up libxv-dev:amd64 (2:1.0.11-1) ...\n",
            "Setting up libcapnp-0.6.1:amd64 (0.6.1-1ubuntu1) ...\n",
            "Setting up x11proto-resource-dev (2018.4-4) ...\n",
            "Setting up x11proto-xf86dga-dev (2018.4-4) ...\n",
            "Setting up libfs6:amd64 (2:1.0.7-1) ...\n",
            "Setting up libxxf86dga-dev:amd64 (2:1.1.4-1) ...\n",
            "Setting up libibus-1.0-5:amd64 (1.5.17-3ubuntu5.3) ...\n",
            "Setting up libdmx1:amd64 (1:1.1.3-1) ...\n",
            "Setting up x11proto-render-dev (2:2018.4-4) ...\n",
            "Setting up libmircore1:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up libudev-dev:amd64 (237-3ubuntu10.53) ...\n",
            "Setting up x11proto-composite-dev (1:2018.4-4) ...\n",
            "Setting up gir1.2-ibus-1.0:amd64 (1.5.17-3ubuntu5.3) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.1-1) ...\n",
            "Setting up libxcomposite-dev:amd64 (1:0.4.4-2) ...\n",
            "Setting up libxaw7-dev:amd64 (2:1.0.13-1) ...\n",
            "Setting up libmirprotobuf3:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libdmx-dev:amd64 (1:1.1.3-1) ...\n",
            "Setting up libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up libxvmc-dev:amd64 (2:1.0.10-1) ...\n",
            "Setting up libmircookie-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libxfont-dev (1:2.0.3-1) ...\n",
            "Setting up libibus-1.0-dev:amd64 (1.5.17-3ubuntu5.3) ...\n",
            "Setting up libfs-dev:amd64 (2:1.0.7-1) ...\n",
            "Setting up libmircommon7:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libmirclient9:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libmircommon-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libmirclient-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libsdl2-dev:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.4) ...\n",
            "Setting up mir-client-platform-mesa-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up xserver-xorg-dev (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xorg-dev (1:7.7+19ubuntu7.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.5.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 101 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.17.3)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (4.1.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.5.3)\n",
            "Installing collected packages: pygame, tf-agents, pyvirtualdisplay\n",
            "Successfully installed pygame-2.1.0 pyvirtualdisplay-3.0 tf-agents-0.12.1\n",
            "Requirement already satisfied: gym[accept-rom-license,atari,box2d] in /usr/local/lib/python3.7/dist-packages (0.23.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (0.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (1.3.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (2.1.0)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d_py-2.3.5-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting ale-py~=0.7.4\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.4->gym[accept-rom-license,atari,box2d]) (5.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym[accept-rom-license,atari,box2d]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym[accept-rom-license,atari,box2d]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=373bdbb98246db32f9f7c3b94b3cb04ef42695b91efc9483da06cafb389a669a\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, box2d-py, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2 box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB or IS_KAGGLE:\n",
        "    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "    %pip install -U tf-agents pyvirtualdisplay\n",
        "    %pip install -U gym>=0.21.0\n",
        "    %pip install -U gym[box2d,atari,accept-rom-license]\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# To get smooth animations\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"rl\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was inspired by using a combination of labs, ray website, https://colab.research.google.com/github/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb and https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"
      ],
      "metadata": {
        "id": "wZNuHGgmPLeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ray==1.12.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r2s6tMwcWxK",
        "outputId": "8d55e98d-e1f1-4678-d263-7ac54f45725d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray==1.12.0\n",
            "  Downloading ray-1.12.0-cp37-cp37m-manylinux2014_x86_64.whl (53.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 53.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (3.6.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (4.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (1.21.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (1.0.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (21.4.0)\n",
            "Collecting grpcio<=1.43.0,>=1.28.1\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 34.8 MB/s \n",
            "\u001b[?25hCollecting frozenlist\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.0 MB/s \n",
            "\u001b[?25hCollecting virtualenv\n",
            "  Downloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 36.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (3.13)\n",
            "Collecting aiosignal\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray==1.12.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray==1.12.0) (1.15.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==1.12.0) (5.7.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==1.12.0) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==1.12.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==1.12.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray==1.12.0) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.12.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.12.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.12.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.12.0) (1.24.3)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 47.8 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: platformdirs, frozenlist, distlib, virtualenv, grpcio, aiosignal, ray\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.44.0\n",
            "    Uninstalling grpcio-1.44.0:\n",
            "      Successfully uninstalled grpcio-1.44.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed aiosignal-1.2.0 distlib-0.3.4 frozenlist-1.3.0 grpcio-1.43.0 platformdirs-2.5.2 ray-1.12.0 virtualenv-20.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import ray\n",
        "ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total"
      ],
      "metadata": {
        "id": "aUV8h3aecSxe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym==0.22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vQOQilIcbmC",
        "outputId": "a07ae93d-d1fa-4d96-a044-ed8cb1f52e18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.22\n",
            "  Downloading gym-0.22.0.tar.gz (631 kB)\n",
            "\u001b[K     |████████████████████████████████| 631 kB 5.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.22) (0.0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.22) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.22) (4.11.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.22) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym==0.22) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym==0.22) (3.8.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.22.0-py3-none-any.whl size=708393 sha256=61e904c912c8ed730b61a47f6e775c7259094ab5a57cf6ae3e7a1dba1ea71331\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/5e/87/7d50e0179edda70feff5bba05c381041e1c1fd80c6b06a4cc3\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.23.1\n",
            "    Uninstalling gym-0.23.1:\n",
            "      Successfully uninstalled gym-0.23.1\n",
            "Successfully installed gym-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U tensorboardx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5g3qBNccdJR",
        "outputId": "02f83f45-50dc-4d69-aff6-15d68efdd4d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.15.0)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lz4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYb-iW3Ccf7q",
        "outputId": "9af2f55d-f022-4400-a744-2ca976416a08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lz4\n",
            "  Downloading lz4-4.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 122 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 143 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 194 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 215 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 245 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 286 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 296 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 317 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 337 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 358 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 368 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 389 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 409 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 430 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 440 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 460 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 471 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 481 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 491 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 501 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 512 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 542 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 552 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 563 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 573 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 583 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 593 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 604 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 614 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 624 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 634 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 645 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 655 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 665 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 675 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 696 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 706 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 716 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 727 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 737 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 747 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 757 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 768 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 778 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 788 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 808 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 819 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 829 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 839 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 849 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 860 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 870 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 880 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 890 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 901 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 911 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 921 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 931 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 942 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 962 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 972 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 983 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 993 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.0 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.0 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.0 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.2 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.2 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 5.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: lz4\n",
            "Successfully installed lz4-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw4fPsC4chn_",
        "outputId": "23719531-fae0-4a97-ffe7-af5241f742d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:506: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
            "  f\"The environment {path} is out of date. You should consider \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "\n",
        "for i in range(20):\n",
        "\n",
        "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
        "  observation, reward, done, info = env.step(env.action_space.sample())\n",
        "\n",
        "  print(\"step\", i, observation, reward, done, info)\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxBcgVEWclSJ",
        "outputId": "8dad401f-d650-4637-ef1c-a0f162267b60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 [ 0.04664953 -0.15030913 -0.0198714   0.3197851 ] 1.0 False {}\n",
            "step 1 [ 0.04364334 -0.3451425  -0.0134757   0.60613567] 1.0 False {}\n",
            "step 2 [ 0.03674049 -0.14983477 -0.00135299  0.30923888] 1.0 False {}\n",
            "step 3 [ 0.0337438  -0.3449374   0.00483179  0.6014948 ] 1.0 False {}\n",
            "step 4 [ 0.02684505 -0.14988337  0.01686169  0.31033772] 1.0 False {}\n",
            "step 5 [ 0.02384738 -0.34524146  0.02306844  0.60829026] 1.0 False {}\n",
            "step 6 [ 0.01694255 -0.5406782   0.03523425  0.9081489 ] 1.0 False {}\n",
            "step 7 [ 0.00612899 -0.7362589   0.05339722  1.2116946 ] 1.0 False {}\n",
            "step 8 [-0.00859619 -0.5418654   0.07763112  0.9362113 ] 1.0 False {}\n",
            "step 9 [-0.0194335  -0.7379436   0.09635534  1.2522434 ] 1.0 False {}\n",
            "step 10 [-0.03419237 -0.544179    0.12140021  0.9912294 ] 1.0 False {}\n",
            "step 11 [-0.04507595 -0.74069804  0.1412248   1.3194424 ] 1.0 False {}\n",
            "step 12 [-0.05988991 -0.54761547  0.16761364  1.0740842 ] 1.0 False {}\n",
            "step 13 [-0.07084222 -0.74450773  0.18909533  1.4143316 ] 1.0 False {}\n",
            "step 14 [-0.08573237 -0.94140154  0.21738195  1.759671  ] 1.0 True {}\n",
            "step 15 [-0.10456041 -1.1382223   0.25257537  2.1113622 ] 0.0 True {}\n",
            "step 16 [-0.12732485 -1.3348292   0.29480264  2.4703858 ] 0.0 True {}\n",
            "step 17 [-0.15402144 -1.1431485   0.34421036  2.2806907 ] 0.0 True {}\n",
            "step 18 [-0.17688441 -0.95245874  0.38982415  2.1106453 ] 0.0 True {}\n",
            "step 19 [-0.19593358 -1.1488944   0.43203706  2.4949205 ] 0.0 True {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py:161: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  \"You are calling 'step()' even though this \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies needed for recording videos\n",
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install pyvirtualdisplay==0.2.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EQVhBkWdBIA",
        "outputId": "17708572-cf49-450c-d4d8-823606934884"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  x11-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 196 kB of archives.\n",
            "After this operation, 650 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Fetched 196 kB in 1s (276 kB/s)\n",
            "Selecting previously unselected package x11-utils.\n",
            "(Reading database ... 157592 files and directories currently installed.)\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "  Attempting uninstall: pyvirtualdisplay\n",
            "    Found existing installation: PyVirtualDisplay 3.0\n",
            "    Uninstalling PyVirtualDisplay-3.0:\n",
            "      Successfully uninstalled PyVirtualDisplay-3.0\n",
            "Successfully installed EasyProcess-1.1 pyvirtualdisplay-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()"
      ],
      "metadata": {
        "id": "GZ-ntygZdFdH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "before_training = \"before_training.mp4\"\n",
        "\n",
        "video = VideoRecorder(env, before_training)\n",
        "# returns an initial observation\n",
        "env.reset()\n",
        "for i in range(200):\n",
        "  video.capture_frame()\n",
        "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
        "  observation, reward, done, info = env.step(env.action_space.sample())\n",
        "  # Not printing this time\n",
        "  #print(\"step\", i, observation, reward, done, info)\n",
        "\n",
        "video.close()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZbFmY8edJhT",
        "outputId": "c247a8e9-21e0-43db-d556-ed62ad7833f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py:161: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  \"You are calling 'step()' even though this \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from base64 import b64encode\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n"
      ],
      "metadata": {
        "id": "NEU1BHUadKPr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.display import HTML\n",
        "html = render_mp4(before_training)\n",
        "HTML(html)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "Hf8IsAE6darB",
        "outputId": "1891bf22-5eeb-41f8-bd41-b011df4f3970"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "config = {\n",
        "    \"env\": \"CartPole-v0\",\n",
        "    # Change the following line to `“framework”: “tf”` to use tensorflow\n",
        "    \"framework\": \"torch\",\n",
        "    \"model\": {\n",
        "      \"fcnet_hiddens\": [32],\n",
        "      \"fcnet_activation\": \"linear\",\n",
        "    },\n",
        "}\n",
        "stop = {\"episode_reward_mean\": 195}\n",
        "ray.shutdown()\n",
        "ray.init(\n",
        "  num_cpus=3,\n",
        "  include_dashboard=False,\n",
        "  ignore_reinit_error=True,\n",
        "  log_to_driver=False,\n",
        ")\n",
        "# execute training \n",
        "analysis = ray.tune.run(\n",
        "  \"PPO\",\n",
        "  config=config,\n",
        "  stop=stop,\n",
        "  checkpoint_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ov98V2uMds00",
        "outputId": "a815fe72-73e4-4568-f0aa-57b802944b61"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-24 09:11:19,748\tINFO trial_runner.py:803 -- starting PPO_CartPole-v0_80d83_00000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:11:35 (running for 00:00:16.29)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:11:40 (running for 00:00:21.31)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 4000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-11-44\n",
            "  done: false\n",
            "  episode_len_mean: 23.156976744186046\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 69.0\n",
            "  episode_reward_mean: 23.156976744186046\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 172\n",
            "  episodes_total: 172\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.20000000000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6839196100029894\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.009778247770419302\n",
            "          policy_loss: -0.020816732550500542\n",
            "          total_loss: 9.190495002910655\n",
            "          vf_explained_var: -0.0009381812105896652\n",
            "          vf_loss: 9.209356138783116\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 4000\n",
            "    num_agent_steps_trained: 4000\n",
            "    num_steps_sampled: 4000\n",
            "    num_steps_trained: 4000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 80.12307692307692\n",
            "    ram_util_percent: 26.38461538461538\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1183241858951811\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14970714839799948\n",
            "    mean_inference_ms: 1.834886661474255\n",
            "    mean_raw_obs_processing_ms: 0.23906991101693414\n",
            "  time_since_restore: 8.631113767623901\n",
            "  time_this_iter_s: 8.631113767623901\n",
            "  time_total_s: 8.631113767623901\n",
            "  timers:\n",
            "    learn_throughput: 1045.094\n",
            "    learn_time_ms: 3827.407\n",
            "    load_throughput: 11177359.094\n",
            "    load_time_ms: 0.358\n",
            "    sample_throughput: 833.27\n",
            "    sample_time_ms: 4800.364\n",
            "    update_time_ms: 1.812\n",
            "  timestamp: 1650791504\n",
            "  timesteps_since_restore: 4000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 4000\n",
            "  training_iteration: 1\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:11:49 (running for 00:00:29.96)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.63111</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  23.157</td><td style=\"text-align: right;\">                  69</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            23.157</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 8000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-11-53\n",
            "  done: false\n",
            "  episode_len_mean: 28.46808510638298\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 113.0\n",
            "  episode_reward_mean: 28.46808510638298\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 141\n",
            "  episodes_total: 313\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.20000000000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6581101596355439\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00839860154657178\n",
            "          policy_loss: -0.020951973316409897\n",
            "          total_loss: 9.28779222426876\n",
            "          vf_explained_var: 0.00039148574234336935\n",
            "          vf_loss: 9.30706446555353\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 8000\n",
            "    num_agent_steps_trained: 8000\n",
            "    num_steps_sampled: 8000\n",
            "    num_steps_trained: 8000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.31666666666668\n",
            "    ram_util_percent: 26.399999999999995\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11620198560527135\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14863662422403304\n",
            "    mean_inference_ms: 1.8224188322791377\n",
            "    mean_raw_obs_processing_ms: 0.23151964203252343\n",
            "  time_since_restore: 17.162699699401855\n",
            "  time_this_iter_s: 8.531585931777954\n",
            "  time_total_s: 17.162699699401855\n",
            "  timers:\n",
            "    learn_throughput: 1042.994\n",
            "    learn_time_ms: 3835.114\n",
            "    load_throughput: 12436779.837\n",
            "    load_time_ms: 0.322\n",
            "    sample_throughput: 598.964\n",
            "    sample_time_ms: 6678.196\n",
            "    update_time_ms: 1.868\n",
            "  timestamp: 1650791513\n",
            "  timesteps_since_restore: 8000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 8000\n",
            "  training_iteration: 2\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:11:58 (running for 00:00:38.60)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         17.1627</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 28.4681</td><td style=\"text-align: right;\">                 113</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           28.4681</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 12000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-12-01\n",
            "  done: false\n",
            "  episode_len_mean: 36.716981132075475\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 105.0\n",
            "  episode_reward_mean: 36.716981132075475\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 106\n",
            "  episodes_total: 419\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.20000000000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6318089772296208\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0062570579861347\n",
            "          policy_loss: -0.01412542988696406\n",
            "          total_loss: 9.374547854802941\n",
            "          vf_explained_var: -0.0026112207802393103\n",
            "          vf_loss: 9.387421836647937\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 12000\n",
            "    num_agent_steps_trained: 12000\n",
            "    num_steps_sampled: 12000\n",
            "    num_steps_trained: 12000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.04166666666667\n",
            "    ram_util_percent: 26.399999999999995\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11605846088881967\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14821047984590444\n",
            "    mean_inference_ms: 1.8128106430016584\n",
            "    mean_raw_obs_processing_ms: 0.2258457439631889\n",
            "  time_since_restore: 25.659103870391846\n",
            "  time_this_iter_s: 8.49640417098999\n",
            "  time_total_s: 25.659103870391846\n",
            "  timers:\n",
            "    learn_throughput: 1043.635\n",
            "    learn_time_ms: 3832.757\n",
            "    load_throughput: 13493739.41\n",
            "    load_time_ms: 0.296\n",
            "    sample_throughput: 547.872\n",
            "    sample_time_ms: 7300.975\n",
            "    update_time_ms: 1.937\n",
            "  timestamp: 1650791521\n",
            "  timesteps_since_restore: 12000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 12000\n",
            "  training_iteration: 3\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:12:06 (running for 00:00:47.11)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         25.6591</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  36.717</td><td style=\"text-align: right;\">                 105</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            36.717</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 16000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-12-10\n",
            "  done: false\n",
            "  episode_len_mean: 52.82\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 52.82\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 66\n",
            "  episodes_total: 485\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.20000000000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6108681246798525\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004364847846616408\n",
            "          policy_loss: -0.0177167674246174\n",
            "          total_loss: 9.511695763885333\n",
            "          vf_explained_var: 0.0001603471335544381\n",
            "          vf_loss: 9.528539556072603\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 16000\n",
            "    num_agent_steps_trained: 16000\n",
            "    num_steps_sampled: 16000\n",
            "    num_steps_trained: 16000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.61666666666666\n",
            "    ram_util_percent: 26.399999999999995\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11592325550443461\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1476069706338072\n",
            "    mean_inference_ms: 1.8087571398606308\n",
            "    mean_raw_obs_processing_ms: 0.22338878053436198\n",
            "  time_since_restore: 34.13198733329773\n",
            "  time_this_iter_s: 8.472883462905884\n",
            "  time_total_s: 34.13198733329773\n",
            "  timers:\n",
            "    learn_throughput: 1044.485\n",
            "    learn_time_ms: 3829.639\n",
            "    load_throughput: 13617870.13\n",
            "    load_time_ms: 0.294\n",
            "    sample_throughput: 525.937\n",
            "    sample_time_ms: 7605.48\n",
            "    update_time_ms: 2.562\n",
            "  timestamp: 1650791530\n",
            "  timesteps_since_restore: 16000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 16000\n",
            "  training_iteration: 4\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:12:15 (running for 00:00:55.57)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          34.132</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   52.82</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             52.82</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 20000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-12-19\n",
            "  done: false\n",
            "  episode_len_mean: 78.52\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 78.52\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 34\n",
            "  episodes_total: 519\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.10000000000000002\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6066805481269796\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0020329963465903697\n",
            "          policy_loss: -0.010770571940848904\n",
            "          total_loss: 9.668381600738854\n",
            "          vf_explained_var: -0.011401888055186118\n",
            "          vf_loss: 9.678948874114662\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 20000\n",
            "    num_agent_steps_trained: 20000\n",
            "    num_steps_sampled: 20000\n",
            "    num_steps_trained: 20000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 80.77692307692308\n",
            "    ram_util_percent: 26.176923076923067\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11602236089805862\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14831269976614003\n",
            "    mean_inference_ms: 1.814822597135037\n",
            "    mean_raw_obs_processing_ms: 0.22012483631516486\n",
            "  time_since_restore: 43.104673624038696\n",
            "  time_this_iter_s: 8.972686290740967\n",
            "  time_total_s: 43.104673624038696\n",
            "  timers:\n",
            "    learn_throughput: 1029.174\n",
            "    learn_time_ms: 3886.61\n",
            "    load_throughput: 13545305.991\n",
            "    load_time_ms: 0.295\n",
            "    sample_throughput: 511.089\n",
            "    sample_time_ms: 7826.42\n",
            "    update_time_ms: 2.448\n",
            "  timestamp: 1650791539\n",
            "  timesteps_since_restore: 20000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 20000\n",
            "  training_iteration: 5\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:12:24 (running for 00:01:04.61)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         43.1047</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   78.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             78.52</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 24000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-12-27\n",
            "  done: false\n",
            "  episode_len_mean: 100.59\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 100.59\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 29\n",
            "  episodes_total: 548\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.05000000000000001\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5995812813440958\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0020656794005375653\n",
            "          policy_loss: -0.014189997303389735\n",
            "          total_loss: 9.720596438582225\n",
            "          vf_explained_var: 0.003428938183733212\n",
            "          vf_loss: 9.73468310756068\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 24000\n",
            "    num_agent_steps_trained: 24000\n",
            "    num_steps_sampled: 24000\n",
            "    num_steps_trained: 24000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.925\n",
            "    ram_util_percent: 25.899999999999995\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11644278219429788\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14827880908057398\n",
            "    mean_inference_ms: 1.8168562729695399\n",
            "    mean_raw_obs_processing_ms: 0.2192037497616648\n",
            "  time_since_restore: 51.53122854232788\n",
            "  time_this_iter_s: 8.426554918289185\n",
            "  time_total_s: 51.53122854232788\n",
            "  timers:\n",
            "    learn_throughput: 1032.465\n",
            "    learn_time_ms: 3874.223\n",
            "    load_throughput: 13748060.093\n",
            "    load_time_ms: 0.291\n",
            "    sample_throughput: 501.073\n",
            "    sample_time_ms: 7982.872\n",
            "    update_time_ms: 2.337\n",
            "  timestamp: 1650791547\n",
            "  timesteps_since_restore: 24000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 24000\n",
            "  training_iteration: 6\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:12:32 (running for 00:01:13.08)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         51.5312</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  100.59</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            100.59</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 28000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-12-36\n",
            "  done: false\n",
            "  episode_len_mean: 125.56\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 125.56\n",
            "  episode_reward_min: 19.0\n",
            "  episodes_this_iter: 24\n",
            "  episodes_total: 572\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.025000000000000005\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5941716926713143\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0024069850937424096\n",
            "          policy_loss: -0.013166029072336612\n",
            "          total_loss: 9.724966759835521\n",
            "          vf_explained_var: -0.0068615899291089785\n",
            "          vf_loss: 9.73807262707782\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 28000\n",
            "    num_agent_steps_trained: 28000\n",
            "    num_steps_sampled: 28000\n",
            "    num_steps_trained: 28000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.74166666666666\n",
            "    ram_util_percent: 25.899999999999995\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11662836176062552\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1484674916260094\n",
            "    mean_inference_ms: 1.8192193534858092\n",
            "    mean_raw_obs_processing_ms: 0.21684673217321673\n",
            "  time_since_restore: 60.025057554244995\n",
            "  time_this_iter_s: 8.493829011917114\n",
            "  time_total_s: 60.025057554244995\n",
            "  timers:\n",
            "    learn_throughput: 1033.543\n",
            "    learn_time_ms: 3870.182\n",
            "    load_throughput: 13114518.37\n",
            "    load_time_ms: 0.305\n",
            "    sample_throughput: 496.545\n",
            "    sample_time_ms: 8055.669\n",
            "    update_time_ms: 2.242\n",
            "  timestamp: 1650791556\n",
            "  timesteps_since_restore: 28000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 28000\n",
            "  training_iteration: 7\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:12:41 (running for 00:01:21.55)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         60.0251</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  125.56</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            125.56</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 32000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-12-44\n",
            "  done: false\n",
            "  episode_len_mean: 143.11\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 143.11\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 26\n",
            "  episodes_total: 598\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.012500000000000002\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.586487994142758\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0007287884865551821\n",
            "          policy_loss: -0.011276548745371\n",
            "          total_loss: 9.690420924976308\n",
            "          vf_explained_var: 0.002576983167279151\n",
            "          vf_loss: 9.701688363475185\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 32000\n",
            "    num_agent_steps_trained: 32000\n",
            "    num_steps_sampled: 32000\n",
            "    num_steps_trained: 32000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.27499999999999\n",
            "    ram_util_percent: 25.924999999999997\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11664115010244303\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14799294137758878\n",
            "    mean_inference_ms: 1.8184322105639705\n",
            "    mean_raw_obs_processing_ms: 0.2148768561190176\n",
            "  time_since_restore: 68.4377019405365\n",
            "  time_this_iter_s: 8.412644386291504\n",
            "  time_total_s: 68.4377019405365\n",
            "  timers:\n",
            "    learn_throughput: 1035.489\n",
            "    learn_time_ms: 3862.907\n",
            "    load_throughput: 13040976.292\n",
            "    load_time_ms: 0.307\n",
            "    sample_throughput: 493.32\n",
            "    sample_time_ms: 8108.329\n",
            "    update_time_ms: 2.264\n",
            "  timestamp: 1650791564\n",
            "  timesteps_since_restore: 32000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 32000\n",
            "  training_iteration: 8\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:12:49 (running for 00:01:30.04)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         68.4377</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  143.11</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            143.11</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 36000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-12-52\n",
            "  done: false\n",
            "  episode_len_mean: 158.46\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 158.46\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 620\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.006250000000000001\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5851640323797862\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002799852460836454\n",
            "          policy_loss: -0.011259971660191333\n",
            "          total_loss: 9.697878653003324\n",
            "          vf_explained_var: 0.005126863269395726\n",
            "          vf_loss: 9.709121132922428\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 36000\n",
            "    num_agent_steps_trained: 36000\n",
            "    num_steps_sampled: 36000\n",
            "    num_steps_trained: 36000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.99166666666667\n",
            "    ram_util_percent: 25.899999999999995\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11674154471136909\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14788786144432833\n",
            "    mean_inference_ms: 1.8162435196456304\n",
            "    mean_raw_obs_processing_ms: 0.21241496169232818\n",
            "  time_since_restore: 76.80150628089905\n",
            "  time_this_iter_s: 8.363804340362549\n",
            "  time_total_s: 76.80150628089905\n",
            "  timers:\n",
            "    learn_throughput: 1038.495\n",
            "    learn_time_ms: 3851.728\n",
            "    load_throughput: 12810294.731\n",
            "    load_time_ms: 0.312\n",
            "    sample_throughput: 491.035\n",
            "    sample_time_ms: 8146.051\n",
            "    update_time_ms: 2.273\n",
            "  timestamp: 1650791572\n",
            "  timesteps_since_restore: 36000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 36000\n",
            "  training_iteration: 9\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:12:58 (running for 00:01:38.46)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         76.8015</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  158.46</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            158.46</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 40000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-13-01\n",
            "  done: false\n",
            "  episode_len_mean: 168.14\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 168.14\n",
            "  episode_reward_min: 31.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 642\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0031250000000000006\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5794236979176921\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0016919468924543849\n",
            "          policy_loss: -0.011348220141183946\n",
            "          total_loss: 9.67035953767838\n",
            "          vf_explained_var: -2.1851383229737642e-05\n",
            "          vf_loss: 9.681702500004922\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 40000\n",
            "    num_agent_steps_trained: 40000\n",
            "    num_steps_sampled: 40000\n",
            "    num_steps_trained: 40000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.35384615384615\n",
            "    ram_util_percent: 25.93846153846154\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1166041450258173\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14750712709384214\n",
            "    mean_inference_ms: 1.8141124192604718\n",
            "    mean_raw_obs_processing_ms: 0.21095502234599045\n",
            "  time_since_restore: 85.39327597618103\n",
            "  time_this_iter_s: 8.591769695281982\n",
            "  time_total_s: 85.39327597618103\n",
            "  timers:\n",
            "    learn_throughput: 1034.584\n",
            "    learn_time_ms: 3866.289\n",
            "    load_throughput: 12890676.911\n",
            "    load_time_ms: 0.31\n",
            "    sample_throughput: 489.499\n",
            "    sample_time_ms: 8171.628\n",
            "    update_time_ms: 2.222\n",
            "  timestamp: 1650791581\n",
            "  timesteps_since_restore: 40000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 40000\n",
            "  training_iteration: 10\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:13:06 (running for 00:01:47.04)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         85.3933</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  168.14</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  31</td><td style=\"text-align: right;\">            168.14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 44000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-13-10\n",
            "  done: false\n",
            "  episode_len_mean: 173.72\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 173.72\n",
            "  episode_reward_min: 31.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 664\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0015625000000000003\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5790254623659196\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00040699922062195057\n",
            "          policy_loss: -0.009263232823020669\n",
            "          total_loss: 9.656951559743574\n",
            "          vf_explained_var: 0.0009052336856883059\n",
            "          vf_loss: 9.666214212807276\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 44000\n",
            "    num_agent_steps_trained: 44000\n",
            "    num_steps_sampled: 44000\n",
            "    num_steps_trained: 44000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.09166666666668\n",
            "    ram_util_percent: 25.991666666666664\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11650908207584049\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14720640477291674\n",
            "    mean_inference_ms: 1.8122747350671033\n",
            "    mean_raw_obs_processing_ms: 0.20971112097180217\n",
            "  time_since_restore: 93.88131523132324\n",
            "  time_this_iter_s: 8.488039255142212\n",
            "  time_total_s: 93.88131523132324\n",
            "  timers:\n",
            "    learn_throughput: 1034.158\n",
            "    learn_time_ms: 3867.88\n",
            "    load_throughput: 11746282.994\n",
            "    load_time_ms: 0.341\n",
            "    sample_throughput: 467.325\n",
            "    sample_time_ms: 8559.345\n",
            "    update_time_ms: 2.22\n",
            "  timestamp: 1650791590\n",
            "  timesteps_since_restore: 44000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 44000\n",
            "  training_iteration: 11\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:13:15 (running for 00:01:55.65)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         93.8813</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  173.72</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  31</td><td style=\"text-align: right;\">            173.72</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 48000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-13-18\n",
            "  done: false\n",
            "  episode_len_mean: 178.77\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 178.77\n",
            "  episode_reward_min: 31.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 685\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0007812500000000002\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.56957894441902\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0015169642715566487\n",
            "          policy_loss: -0.01115473327777719\n",
            "          total_loss: 9.671778877832557\n",
            "          vf_explained_var: 0.004163157747637841\n",
            "          vf_loss: 9.682932447618054\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 48000\n",
            "    num_agent_steps_trained: 48000\n",
            "    num_steps_sampled: 48000\n",
            "    num_steps_trained: 48000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.51666666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11644317740707401\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14698128310541247\n",
            "    mean_inference_ms: 1.8113603337158848\n",
            "    mean_raw_obs_processing_ms: 0.2089066446005659\n",
            "  time_since_restore: 102.4242217540741\n",
            "  time_this_iter_s: 8.542906522750854\n",
            "  time_total_s: 102.4242217540741\n",
            "  timers:\n",
            "    learn_throughput: 1034.637\n",
            "    learn_time_ms: 3866.09\n",
            "    load_throughput: 11715114.866\n",
            "    load_time_ms: 0.341\n",
            "    sample_throughput: 467.11\n",
            "    sample_time_ms: 8563.293\n",
            "    update_time_ms: 2.299\n",
            "  timestamp: 1650791598\n",
            "  timesteps_since_restore: 48000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 48000\n",
            "  training_iteration: 12\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:13:23 (running for 00:02:04.20)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         102.424</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  178.77</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  31</td><td style=\"text-align: right;\">            178.77</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 52000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-13-27\n",
            "  done: false\n",
            "  episode_len_mean: 186.14\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 186.14\n",
            "  episode_reward_min: 36.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 706\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.0003906250000000001\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5753160252366015\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003192505415115012\n",
            "          policy_loss: -0.011150707923857275\n",
            "          total_loss: 9.659635301815566\n",
            "          vf_explained_var: -0.02457510002197758\n",
            "          vf_loss: 9.67078477695424\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 52000\n",
            "    num_agent_steps_trained: 52000\n",
            "    num_steps_sampled: 52000\n",
            "    num_steps_trained: 52000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.35833333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11646425547892282\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14696070363477667\n",
            "    mean_inference_ms: 1.8109664020178728\n",
            "    mean_raw_obs_processing_ms: 0.20783447125071633\n",
            "  time_since_restore: 110.82749104499817\n",
            "  time_this_iter_s: 8.403269290924072\n",
            "  time_total_s: 110.82749104499817\n",
            "  timers:\n",
            "    learn_throughput: 1036.603\n",
            "    learn_time_ms: 3858.759\n",
            "    load_throughput: 11596886.708\n",
            "    load_time_ms: 0.345\n",
            "    sample_throughput: 467.308\n",
            "    sample_time_ms: 8559.661\n",
            "    update_time_ms: 2.349\n",
            "  timestamp: 1650791607\n",
            "  timesteps_since_restore: 52000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 52000\n",
            "  training_iteration: 13\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:13:32 (running for 00:02:12.58)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         110.827</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  186.14</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">            186.14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 56000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-13-35\n",
            "  done: false\n",
            "  episode_len_mean: 189.26\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 189.26\n",
            "  episode_reward_min: 36.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 726\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 0.00019531250000000004\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5821328469502028\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002013003285206207\n",
            "          policy_loss: -0.011152190756132847\n",
            "          total_loss: 9.659624739616149\n",
            "          vf_explained_var: -0.02332429321863318\n",
            "          vf_loss: 9.670776602016982\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 56000\n",
            "    num_agent_steps_trained: 56000\n",
            "    num_steps_sampled: 56000\n",
            "    num_steps_trained: 56000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.35000000000001\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11652134270397414\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1468685410630082\n",
            "    mean_inference_ms: 1.8109905188764595\n",
            "    mean_raw_obs_processing_ms: 0.2072134881123517\n",
            "  time_since_restore: 119.29683232307434\n",
            "  time_this_iter_s: 8.469341278076172\n",
            "  time_total_s: 119.29683232307434\n",
            "  timers:\n",
            "    learn_throughput: 1037.888\n",
            "    learn_time_ms: 3853.979\n",
            "    load_throughput: 11608923.332\n",
            "    load_time_ms: 0.345\n",
            "    sample_throughput: 467.471\n",
            "    sample_time_ms: 8556.681\n",
            "    update_time_ms: 2.171\n",
            "  timestamp: 1650791615\n",
            "  timesteps_since_restore: 56000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 56000\n",
            "  training_iteration: 14\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:13:40 (running for 00:02:21.17)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         119.297</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  189.26</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">            189.26</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 60000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-13-44\n",
            "  done: false\n",
            "  episode_len_mean: 186.4\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 186.4\n",
            "  episode_reward_min: 26.0\n",
            "  episodes_this_iter: 23\n",
            "  episodes_total: 749\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.765625000000002e-05\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5622739823274715\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002663161546694063\n",
            "          policy_loss: -0.012205908185131448\n",
            "          total_loss: 9.638259566727506\n",
            "          vf_explained_var: 0.00523521868131494\n",
            "          vf_loss: 9.65046529872443\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 60000\n",
            "    num_agent_steps_trained: 60000\n",
            "    num_steps_sampled: 60000\n",
            "    num_steps_trained: 60000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.77692307692308\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11664934209790838\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14684553033363998\n",
            "    mean_inference_ms: 1.8120486296691831\n",
            "    mean_raw_obs_processing_ms: 0.2067640959738997\n",
            "  time_since_restore: 127.88820552825928\n",
            "  time_this_iter_s: 8.591373205184937\n",
            "  time_total_s: 127.88820552825928\n",
            "  timers:\n",
            "    learn_throughput: 1046.047\n",
            "    learn_time_ms: 3823.922\n",
            "    load_throughput: 11547398.995\n",
            "    load_time_ms: 0.346\n",
            "    sample_throughput: 468.158\n",
            "    sample_time_ms: 8544.122\n",
            "    update_time_ms: 2.169\n",
            "  timestamp: 1650791624\n",
            "  timesteps_since_restore: 60000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 60000\n",
            "  training_iteration: 15\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:13:49 (running for 00:02:29.76)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         127.888</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">   186.4</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">             186.4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 64000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-13-52\n",
            "  done: false\n",
            "  episode_len_mean: 190.27\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 190.27\n",
            "  episode_reward_min: 26.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 769\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.882812500000001e-05\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5574787519952302\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.001753975090084643\n",
            "          policy_loss: -0.009764933493989771\n",
            "          total_loss: 9.65657841261997\n",
            "          vf_explained_var: 0.005827916758034819\n",
            "          vf_loss: 9.666343340309718\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 64000\n",
            "    num_agent_steps_trained: 64000\n",
            "    num_steps_sampled: 64000\n",
            "    num_steps_trained: 64000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.675\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11675147268202725\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14691075640930454\n",
            "    mean_inference_ms: 1.813196359468235\n",
            "    mean_raw_obs_processing_ms: 0.20627353919355998\n",
            "  time_since_restore: 136.48205971717834\n",
            "  time_this_iter_s: 8.593854188919067\n",
            "  time_total_s: 136.48205971717834\n",
            "  timers:\n",
            "    learn_throughput: 1042.905\n",
            "    learn_time_ms: 3835.439\n",
            "    load_throughput: 11517276.035\n",
            "    load_time_ms: 0.347\n",
            "    sample_throughput: 469.537\n",
            "    sample_time_ms: 8519.03\n",
            "    update_time_ms: 2.212\n",
            "  timestamp: 1650791632\n",
            "  timesteps_since_restore: 64000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 64000\n",
            "  training_iteration: 16\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:13:57 (running for 00:02:38.34)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         136.482</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  190.27</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">            190.27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 68000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-14-01\n",
            "  done: false\n",
            "  episode_len_mean: 191.24\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 191.24\n",
            "  episode_reward_min: 26.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 790\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.4414062500000005e-05\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5503572053165846\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003918760887190871\n",
            "          policy_loss: -0.011244714290143983\n",
            "          total_loss: 9.632140828204411\n",
            "          vf_explained_var: 0.001134013552819529\n",
            "          vf_loss: 9.643385536439958\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 68000\n",
            "    num_agent_steps_trained: 68000\n",
            "    num_steps_sampled: 68000\n",
            "    num_steps_trained: 68000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.8\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11685384859281185\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1469843085350064\n",
            "    mean_inference_ms: 1.8142377936572514\n",
            "    mean_raw_obs_processing_ms: 0.20574824126152313\n",
            "  time_since_restore: 144.95150208473206\n",
            "  time_this_iter_s: 8.469442367553711\n",
            "  time_total_s: 144.95150208473206\n",
            "  timers:\n",
            "    learn_throughput: 1044.792\n",
            "    learn_time_ms: 3828.512\n",
            "    load_throughput: 11455934.449\n",
            "    load_time_ms: 0.349\n",
            "    sample_throughput: 468.642\n",
            "    sample_time_ms: 8535.309\n",
            "    update_time_ms: 2.262\n",
            "  timestamp: 1650791641\n",
            "  timesteps_since_restore: 68000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 68000\n",
            "  training_iteration: 17\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:14:06 (running for 00:02:46.91)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         144.952</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  191.24</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">            191.24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 72000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-14-09\n",
            "  done: false\n",
            "  episode_len_mean: 190.12\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 190.12\n",
            "  episode_reward_min: 26.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 811\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.2207031250000002e-05\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5547392613144331\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002478076267506706\n",
            "          policy_loss: -0.010638751840639499\n",
            "          total_loss: 9.615228980074647\n",
            "          vf_explained_var: -0.006971975359865414\n",
            "          vf_loss: 9.625867731853198\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 72000\n",
            "    num_agent_steps_trained: 72000\n",
            "    num_steps_sampled: 72000\n",
            "    num_steps_trained: 72000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.23333333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11693399522717866\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1470117963019708\n",
            "    mean_inference_ms: 1.8151762359759018\n",
            "    mean_raw_obs_processing_ms: 0.20539240972090325\n",
            "  time_since_restore: 153.3581781387329\n",
            "  time_this_iter_s: 8.406676054000854\n",
            "  time_total_s: 153.3581781387329\n",
            "  timers:\n",
            "    learn_throughput: 1046.154\n",
            "    learn_time_ms: 3823.527\n",
            "    load_throughput: 11650844.444\n",
            "    load_time_ms: 0.343\n",
            "    sample_throughput: 468.783\n",
            "    sample_time_ms: 8532.727\n",
            "    update_time_ms: 2.244\n",
            "  timestamp: 1650791649\n",
            "  timesteps_since_restore: 72000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 72000\n",
            "  training_iteration: 18\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:14:14 (running for 00:02:55.33)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         153.358</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  190.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">            190.12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 76000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-14-18\n",
            "  done: false\n",
            "  episode_len_mean: 191.89\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 191.89\n",
            "  episode_reward_min: 26.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 832\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 6.103515625000001e-06\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5613882101992125\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002601680132097555\n",
            "          policy_loss: -0.010177657599010135\n",
            "          total_loss: 9.609634516316076\n",
            "          vf_explained_var: 0.01579469653867906\n",
            "          vf_loss: 9.619812157333538\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 76000\n",
            "    num_agent_steps_trained: 76000\n",
            "    num_steps_sampled: 76000\n",
            "    num_steps_trained: 76000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.50000000000001\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1169924009827785\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14709033675060604\n",
            "    mean_inference_ms: 1.815790439618093\n",
            "    mean_raw_obs_processing_ms: 0.2049821032764067\n",
            "  time_since_restore: 161.88001132011414\n",
            "  time_this_iter_s: 8.521833181381226\n",
            "  time_total_s: 161.88001132011414\n",
            "  timers:\n",
            "    learn_throughput: 1044.838\n",
            "    learn_time_ms: 3828.344\n",
            "    load_throughput: 11715932.961\n",
            "    load_time_ms: 0.341\n",
            "    sample_throughput: 468.468\n",
            "    sample_time_ms: 8538.478\n",
            "    update_time_ms: 2.252\n",
            "  timestamp: 1650791658\n",
            "  timesteps_since_restore: 76000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 76000\n",
            "  training_iteration: 19\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:14:23 (running for 00:03:03.83)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">          161.88</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  191.89</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">            191.89</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 80000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-14-26\n",
            "  done: false\n",
            "  episode_len_mean: 193.46\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 193.46\n",
            "  episode_reward_min: 126.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 852\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.0517578125000006e-06\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5624343554178873\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 5.71605358428249e-05\n",
            "          policy_loss: -0.007414397085586223\n",
            "          total_loss: 9.640259366394371\n",
            "          vf_explained_var: -0.015875320537115937\n",
            "          vf_loss: 9.647673763767365\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 80000\n",
            "    num_agent_steps_trained: 80000\n",
            "    num_steps_sampled: 80000\n",
            "    num_steps_trained: 80000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.15833333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11703970350762524\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14714655124552062\n",
            "    mean_inference_ms: 1.8161304319303466\n",
            "    mean_raw_obs_processing_ms: 0.20441563066496637\n",
            "  time_since_restore: 170.30168867111206\n",
            "  time_this_iter_s: 8.421677350997925\n",
            "  time_total_s: 170.30168867111206\n",
            "  timers:\n",
            "    learn_throughput: 1050.387\n",
            "    learn_time_ms: 3808.121\n",
            "    load_throughput: 11662182.678\n",
            "    load_time_ms: 0.343\n",
            "    sample_throughput: 468.08\n",
            "    sample_time_ms: 8545.555\n",
            "    update_time_ms: 2.278\n",
            "  timestamp: 1650791666\n",
            "  timesteps_since_restore: 80000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 80000\n",
            "  training_iteration: 20\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:14:32 (running for 00:03:12.37)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         170.302</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  193.46</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 126</td><td style=\"text-align: right;\">            193.46</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 84000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-14-35\n",
            "  done: false\n",
            "  episode_len_mean: 192.24\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.24\n",
            "  episode_reward_min: 122.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 873\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.5258789062500003e-06\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5690332411437906\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00145728788929186\n",
            "          policy_loss: -0.008561740640891335\n",
            "          total_loss: 9.591369492520569\n",
            "          vf_explained_var: 0.008418364037749588\n",
            "          vf_loss: 9.599931235979962\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 84000\n",
            "    num_agent_steps_trained: 84000\n",
            "    num_steps_sampled: 84000\n",
            "    num_steps_trained: 84000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.97500000000001\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11709669205174081\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1471152792311713\n",
            "    mean_inference_ms: 1.8159210055651036\n",
            "    mean_raw_obs_processing_ms: 0.2040048051913997\n",
            "  time_since_restore: 178.68650197982788\n",
            "  time_this_iter_s: 8.38481330871582\n",
            "  time_total_s: 178.68650197982788\n",
            "  timers:\n",
            "    learn_throughput: 1052.252\n",
            "    learn_time_ms: 3801.37\n",
            "    load_throughput: 13228113.222\n",
            "    load_time_ms: 0.302\n",
            "    sample_throughput: 469.405\n",
            "    sample_time_ms: 8521.435\n",
            "    update_time_ms: 2.303\n",
            "  timestamp: 1650791675\n",
            "  timesteps_since_restore: 84000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 84000\n",
            "  training_iteration: 21\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:14:40 (running for 00:03:20.75)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         178.687</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  192.24</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 122</td><td style=\"text-align: right;\">            192.24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 88000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-14-43\n",
            "  done: false\n",
            "  episode_len_mean: 191.59\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 191.59\n",
            "  episode_reward_min: 122.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 894\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.629394531250001e-07\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.567722192566882\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00289199062975686\n",
            "          policy_loss: -0.009148495732956837\n",
            "          total_loss: 9.580978888850058\n",
            "          vf_explained_var: 0.0009784463913209977\n",
            "          vf_loss: 9.590127391199912\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 88000\n",
            "    num_agent_steps_trained: 88000\n",
            "    num_steps_sampled: 88000\n",
            "    num_steps_trained: 88000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.00769230769231\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11715250828406991\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1470328274551668\n",
            "    mean_inference_ms: 1.815672112563021\n",
            "    mean_raw_obs_processing_ms: 0.20365880837168698\n",
            "  time_since_restore: 187.23569297790527\n",
            "  time_this_iter_s: 8.549190998077393\n",
            "  time_total_s: 187.23569297790527\n",
            "  timers:\n",
            "    learn_throughput: 1050.794\n",
            "    learn_time_ms: 3806.644\n",
            "    load_throughput: 13050105.787\n",
            "    load_time_ms: 0.307\n",
            "    sample_throughput: 470.042\n",
            "    sample_time_ms: 8509.873\n",
            "    update_time_ms: 2.211\n",
            "  timestamp: 1650791683\n",
            "  timesteps_since_restore: 88000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 88000\n",
            "  training_iteration: 22\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:14:48 (running for 00:03:29.28)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         187.236</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  191.59</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 122</td><td style=\"text-align: right;\">            191.59</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 92000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-14-52\n",
            "  done: false\n",
            "  episode_len_mean: 189.81\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 189.81\n",
            "  episode_reward_min: 121.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 916\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.814697265625001e-07\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5668606261412302\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0020208417508371895\n",
            "          policy_loss: -0.007697106066650601\n",
            "          total_loss: 9.552147306421752\n",
            "          vf_explained_var: -0.015425552027199857\n",
            "          vf_loss: 9.559844442593153\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 92000\n",
            "    num_agent_steps_trained: 92000\n",
            "    num_steps_sampled: 92000\n",
            "    num_steps_trained: 92000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.575\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11719628314536859\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14695089858514676\n",
            "    mean_inference_ms: 1.8150635535321369\n",
            "    mean_raw_obs_processing_ms: 0.20324573856631403\n",
            "  time_since_restore: 195.58714294433594\n",
            "  time_this_iter_s: 8.351449966430664\n",
            "  time_total_s: 195.58714294433594\n",
            "  timers:\n",
            "    learn_throughput: 1050.439\n",
            "    learn_time_ms: 3807.932\n",
            "    load_throughput: 13036922.838\n",
            "    load_time_ms: 0.307\n",
            "    sample_throughput: 470.144\n",
            "    sample_time_ms: 8508.032\n",
            "    update_time_ms: 2.205\n",
            "  timestamp: 1650791692\n",
            "  timesteps_since_restore: 92000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 92000\n",
            "  training_iteration: 23\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:14:57 (running for 00:03:37.75)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         195.587</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  189.81</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 121</td><td style=\"text-align: right;\">            189.81</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 96000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-15-00\n",
            "  done: false\n",
            "  episode_len_mean: 181.95\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 181.95\n",
            "  episode_reward_min: 73.0\n",
            "  episodes_this_iter: 25\n",
            "  episodes_total: 941\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.9073486328125004e-07\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.558614698533089\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.005347022793602956\n",
            "          policy_loss: -0.01871847453457053\n",
            "          total_loss: 9.596814995427286\n",
            "          vf_explained_var: -0.1288243080339124\n",
            "          vf_loss: 9.615533489309332\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 96000\n",
            "    num_agent_steps_trained: 96000\n",
            "    num_steps_sampled: 96000\n",
            "    num_steps_trained: 96000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.99166666666666\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11716163032827556\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14675047821598666\n",
            "    mean_inference_ms: 1.8142644813874782\n",
            "    mean_raw_obs_processing_ms: 0.2029780555316703\n",
            "  time_since_restore: 204.0228841304779\n",
            "  time_this_iter_s: 8.435741186141968\n",
            "  time_total_s: 204.0228841304779\n",
            "  timers:\n",
            "    learn_throughput: 1050.203\n",
            "    learn_time_ms: 3808.786\n",
            "    load_throughput: 12965391.036\n",
            "    load_time_ms: 0.309\n",
            "    sample_throughput: 470.332\n",
            "    sample_time_ms: 8504.626\n",
            "    update_time_ms: 2.169\n",
            "  timestamp: 1650791700\n",
            "  timesteps_since_restore: 96000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 96000\n",
            "  training_iteration: 24\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:15:05 (running for 00:03:46.19)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         204.023</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  181.95</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">            181.95</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 100000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-15-09\n",
            "  done: false\n",
            "  episode_len_mean: 179.42\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 179.42\n",
            "  episode_reward_min: 73.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 963\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.9073486328125004e-07\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.572360090350592\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00316656577069303\n",
            "          policy_loss: -0.009217159061502386\n",
            "          total_loss: 9.586631885651618\n",
            "          vf_explained_var: -0.02613584373586921\n",
            "          vf_loss: 9.595849066908642\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 100000\n",
            "    num_agent_steps_trained: 100000\n",
            "    num_steps_sampled: 100000\n",
            "    num_steps_trained: 100000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.14999999999999\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11713192457221723\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14669224744847809\n",
            "    mean_inference_ms: 1.8135623386532134\n",
            "    mean_raw_obs_processing_ms: 0.20267090253311132\n",
            "  time_since_restore: 212.35746550559998\n",
            "  time_this_iter_s: 8.33458137512207\n",
            "  time_total_s: 212.35746550559998\n",
            "  timers:\n",
            "    learn_throughput: 1051.375\n",
            "    learn_time_ms: 3804.543\n",
            "    load_throughput: 13202090.022\n",
            "    load_time_ms: 0.303\n",
            "    sample_throughput: 471.473\n",
            "    sample_time_ms: 8484.056\n",
            "    update_time_ms: 2.143\n",
            "  timestamp: 1650791709\n",
            "  timesteps_since_restore: 100000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 100000\n",
            "  training_iteration: 25\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:15:14 (running for 00:03:54.51)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         212.357</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  179.42</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">            179.42</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 104000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-15-17\n",
            "  done: false\n",
            "  episode_len_mean: 178.66\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 178.66\n",
            "  episode_reward_min: 73.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 983\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.536743164062502e-08\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5680396663886245\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0012103082405986456\n",
            "          policy_loss: -0.008785517565825934\n",
            "          total_loss: 9.674835549631426\n",
            "          vf_explained_var: -0.013925858466855942\n",
            "          vf_loss: 9.683621109172861\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 104000\n",
            "    num_agent_steps_trained: 104000\n",
            "    num_steps_sampled: 104000\n",
            "    num_steps_trained: 104000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.10000000000001\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11709574874520676\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14662138407635697\n",
            "    mean_inference_ms: 1.8129181416021323\n",
            "    mean_raw_obs_processing_ms: 0.2024408849674648\n",
            "  time_since_restore: 220.80093908309937\n",
            "  time_this_iter_s: 8.44347357749939\n",
            "  time_total_s: 220.80093908309937\n",
            "  timers:\n",
            "    learn_throughput: 1054.409\n",
            "    learn_time_ms: 3793.593\n",
            "    load_throughput: 13364040.147\n",
            "    load_time_ms: 0.299\n",
            "    sample_throughput: 471.933\n",
            "    sample_time_ms: 8475.783\n",
            "    update_time_ms: 2.137\n",
            "  timestamp: 1650791717\n",
            "  timesteps_since_restore: 104000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 104000\n",
            "  training_iteration: 26\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:15:22 (running for 00:04:03.06)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         220.801</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  178.66</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">            178.66</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 108000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-15-26\n",
            "  done: false\n",
            "  episode_len_mean: 179.49\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 179.49\n",
            "  episode_reward_min: 73.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1005\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.768371582031251e-08\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5691961933207769\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00025335783376250436\n",
            "          policy_loss: -0.006658322848780181\n",
            "          total_loss: 9.583373374323692\n",
            "          vf_explained_var: 0.07805346808125896\n",
            "          vf_loss: 9.590031698698638\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 108000\n",
            "    num_agent_steps_trained: 108000\n",
            "    num_steps_sampled: 108000\n",
            "    num_steps_trained: 108000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.22500000000001\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11707891713750325\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14655896730303386\n",
            "    mean_inference_ms: 1.8122588829119208\n",
            "    mean_raw_obs_processing_ms: 0.20225053126345982\n",
            "  time_since_restore: 229.22976899147034\n",
            "  time_this_iter_s: 8.428829908370972\n",
            "  time_total_s: 229.22976899147034\n",
            "  timers:\n",
            "    learn_throughput: 1054.717\n",
            "    learn_time_ms: 3792.488\n",
            "    load_throughput: 13942670.988\n",
            "    load_time_ms: 0.287\n",
            "    sample_throughput: 472.705\n",
            "    sample_time_ms: 8461.939\n",
            "    update_time_ms: 2.168\n",
            "  timestamp: 1650791726\n",
            "  timesteps_since_restore: 108000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 108000\n",
            "  training_iteration: 27\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:15:31 (running for 00:04:11.51)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">          229.23</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  179.49</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">            179.49</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 112000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-15-34\n",
            "  done: false\n",
            "  episode_len_mean: 182.64\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 182.64\n",
            "  episode_reward_min: 73.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1027\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.3841857910156255e-08\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5647007225021239\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0017369103776414922\n",
            "          policy_loss: -0.007693417276686398\n",
            "          total_loss: 9.558698248094128\n",
            "          vf_explained_var: 0.05542719453893682\n",
            "          vf_loss: 9.566391650579309\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 112000\n",
            "    num_agent_steps_trained: 112000\n",
            "    num_steps_sampled: 112000\n",
            "    num_steps_trained: 112000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.28333333333335\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1170756430359278\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.146538107683151\n",
            "    mean_inference_ms: 1.811758917155493\n",
            "    mean_raw_obs_processing_ms: 0.2021201438229545\n",
            "  time_since_restore: 237.7517819404602\n",
            "  time_this_iter_s: 8.522012948989868\n",
            "  time_total_s: 237.7517819404602\n",
            "  timers:\n",
            "    learn_throughput: 1052.3\n",
            "    learn_time_ms: 3801.199\n",
            "    load_throughput: 13556250.808\n",
            "    load_time_ms: 0.295\n",
            "    sample_throughput: 472.598\n",
            "    sample_time_ms: 8463.846\n",
            "    update_time_ms: 2.175\n",
            "  timestamp: 1650791734\n",
            "  timesteps_since_restore: 112000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 112000\n",
            "  training_iteration: 28\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:15:39 (running for 00:04:20.00)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         237.752</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  182.64</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">            182.64</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 116000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-15-43\n",
            "  done: false\n",
            "  episode_len_mean: 183.56\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 183.56\n",
            "  episode_reward_min: 25.0\n",
            "  episodes_this_iter: 23\n",
            "  episodes_total: 1050\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.1920928955078127e-08\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5602951606755615\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0033025067302891106\n",
            "          policy_loss: -0.007693423643227546\n",
            "          total_loss: 9.506006798692928\n",
            "          vf_explained_var: 0.07515301781315957\n",
            "          vf_loss: 9.513700204254478\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 116000\n",
            "    num_agent_steps_trained: 116000\n",
            "    num_steps_sampled: 116000\n",
            "    num_steps_trained: 116000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 29\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.56666666666668\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11709301462010742\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14657748473188226\n",
            "    mean_inference_ms: 1.8115461600152292\n",
            "    mean_raw_obs_processing_ms: 0.20194390542947296\n",
            "  time_since_restore: 246.2194857597351\n",
            "  time_this_iter_s: 8.467703819274902\n",
            "  time_total_s: 246.2194857597351\n",
            "  timers:\n",
            "    learn_throughput: 1052.465\n",
            "    learn_time_ms: 3800.6\n",
            "    load_throughput: 13579292.594\n",
            "    load_time_ms: 0.295\n",
            "    sample_throughput: 472.359\n",
            "    sample_time_ms: 8468.138\n",
            "    update_time_ms: 2.113\n",
            "  timestamp: 1650791743\n",
            "  timesteps_since_restore: 116000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 116000\n",
            "  training_iteration: 29\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:15:48 (running for 00:04:28.59)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         246.219</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  183.56</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  25</td><td style=\"text-align: right;\">            183.56</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 120000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-15-51\n",
            "  done: false\n",
            "  episode_len_mean: 185.45\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 185.45\n",
            "  episode_reward_min: 25.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1070\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.960464477539064e-09\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5662627904004948\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0002891961488881297\n",
            "          policy_loss: -0.005806333413447744\n",
            "          total_loss: 9.585081822385071\n",
            "          vf_explained_var: 0.010686926623826386\n",
            "          vf_loss: 9.590888161813059\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 120000\n",
            "    num_agent_steps_trained: 120000\n",
            "    num_steps_sampled: 120000\n",
            "    num_steps_trained: 120000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 30\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.15833333333335\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11711571080424805\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14659244380055\n",
            "    mean_inference_ms: 1.8114477205311008\n",
            "    mean_raw_obs_processing_ms: 0.2018154701577582\n",
            "  time_since_restore: 254.69452238082886\n",
            "  time_this_iter_s: 8.47503662109375\n",
            "  time_total_s: 254.69452238082886\n",
            "  timers:\n",
            "    learn_throughput: 1050.112\n",
            "    learn_time_ms: 3809.117\n",
            "    load_throughput: 13595799.028\n",
            "    load_time_ms: 0.294\n",
            "    sample_throughput: 472.585\n",
            "    sample_time_ms: 8464.088\n",
            "    update_time_ms: 2.139\n",
            "  timestamp: 1650791751\n",
            "  timesteps_since_restore: 120000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 120000\n",
            "  training_iteration: 30\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:15:56 (running for 00:04:37.06)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         254.695</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  185.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  25</td><td style=\"text-align: right;\">            185.45</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 124000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-16-00\n",
            "  done: false\n",
            "  episode_len_mean: 186.27\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 186.27\n",
            "  episode_reward_min: 22.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1091\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.980232238769532e-09\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5684262174431995\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0010512359856061939\n",
            "          policy_loss: -0.007796977707735633\n",
            "          total_loss: 9.567833448225452\n",
            "          vf_explained_var: 0.007319008663136472\n",
            "          vf_loss: 9.575630431021413\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 124000\n",
            "    num_agent_steps_trained: 124000\n",
            "    num_steps_sampled: 124000\n",
            "    num_steps_trained: 124000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 31\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.71666666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11710329577736682\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14659474697886504\n",
            "    mean_inference_ms: 1.8113137345878698\n",
            "    mean_raw_obs_processing_ms: 0.2017155398926796\n",
            "  time_since_restore: 263.2552788257599\n",
            "  time_this_iter_s: 8.56075644493103\n",
            "  time_total_s: 263.2552788257599\n",
            "  timers:\n",
            "    learn_throughput: 1046.694\n",
            "    learn_time_ms: 3821.555\n",
            "    load_throughput: 13444359.324\n",
            "    load_time_ms: 0.298\n",
            "    sample_throughput: 471.811\n",
            "    sample_time_ms: 8477.977\n",
            "    update_time_ms: 2.137\n",
            "  timestamp: 1650791760\n",
            "  timesteps_since_restore: 124000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 124000\n",
            "  training_iteration: 31\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:16:05 (running for 00:04:45.61)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         263.255</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  186.27</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            186.27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 128000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-16-08\n",
            "  done: false\n",
            "  episode_len_mean: 188.02\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 188.02\n",
            "  episode_reward_min: 22.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1112\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.490116119384766e-09\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5838359405276596\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.001517433209312169\n",
            "          policy_loss: -0.010925018904550422\n",
            "          total_loss: 9.58956216996716\n",
            "          vf_explained_var: 0.014379340538414576\n",
            "          vf_loss: 9.600487201444563\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 128000\n",
            "    num_agent_steps_trained: 128000\n",
            "    num_steps_sampled: 128000\n",
            "    num_steps_trained: 128000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 32\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.6076923076923\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1170903079435401\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1465950620996898\n",
            "    mean_inference_ms: 1.8112522665138777\n",
            "    mean_raw_obs_processing_ms: 0.20162924379084646\n",
            "  time_since_restore: 271.77506279945374\n",
            "  time_this_iter_s: 8.519783973693848\n",
            "  time_total_s: 271.77506279945374\n",
            "  timers:\n",
            "    learn_throughput: 1047.722\n",
            "    learn_time_ms: 3817.808\n",
            "    load_throughput: 13478923.435\n",
            "    load_time_ms: 0.297\n",
            "    sample_throughput: 471.045\n",
            "    sample_time_ms: 8491.758\n",
            "    update_time_ms: 2.167\n",
            "  timestamp: 1650791768\n",
            "  timesteps_since_restore: 128000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 128000\n",
            "  training_iteration: 32\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:16:13 (running for 00:04:54.28)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         271.775</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">  188.02</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            188.02</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 132000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-16-17\n",
            "  done: false\n",
            "  episode_len_mean: 186.95\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 186.95\n",
            "  episode_reward_min: 22.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1134\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.45058059692383e-10\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5792416502711594\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0016636862316673489\n",
            "          policy_loss: -0.005920381160072421\n",
            "          total_loss: 9.555989306460145\n",
            "          vf_explained_var: 0.03983911968046619\n",
            "          vf_loss: 9.561909707387288\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 132000\n",
            "    num_agent_steps_trained: 132000\n",
            "    num_steps_sampled: 132000\n",
            "    num_steps_trained: 132000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 33\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.9\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11708821459285143\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14661113210473745\n",
            "    mean_inference_ms: 1.8114334144002797\n",
            "    mean_raw_obs_processing_ms: 0.2015489550707313\n",
            "  time_since_restore: 280.3303334712982\n",
            "  time_this_iter_s: 8.555270671844482\n",
            "  time_total_s: 280.3303334712982\n",
            "  timers:\n",
            "    learn_throughput: 1046.378\n",
            "    learn_time_ms: 3822.71\n",
            "    load_throughput: 13496272.223\n",
            "    load_time_ms: 0.296\n",
            "    sample_throughput: 470.324\n",
            "    sample_time_ms: 8504.772\n",
            "    update_time_ms: 2.171\n",
            "  timestamp: 1650791777\n",
            "  timesteps_since_restore: 132000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 132000\n",
            "  training_iteration: 33\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:16:22 (running for 00:05:02.82)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          280.33</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  186.95</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            186.95</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 136000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-16-25\n",
            "  done: false\n",
            "  episode_len_mean: 192.39\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.39\n",
            "  episode_reward_min: 22.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1154\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.725290298461915e-10\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5781540152206216\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0019049488690346837\n",
            "          policy_loss: -0.00526002020705291\n",
            "          total_loss: 9.481499882154568\n",
            "          vf_explained_var: 0.04099181678987319\n",
            "          vf_loss: 9.486759924119518\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 136000\n",
            "    num_agent_steps_trained: 136000\n",
            "    num_steps_sampled: 136000\n",
            "    num_steps_trained: 136000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 34\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.825\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1170921169311741\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14671323381533774\n",
            "    mean_inference_ms: 1.811756621144174\n",
            "    mean_raw_obs_processing_ms: 0.20140953609497955\n",
            "  time_since_restore: 288.88174414634705\n",
            "  time_this_iter_s: 8.551410675048828\n",
            "  time_total_s: 288.88174414634705\n",
            "  timers:\n",
            "    learn_throughput: 1043.708\n",
            "    learn_time_ms: 3832.488\n",
            "    load_throughput: 13417479.207\n",
            "    load_time_ms: 0.298\n",
            "    sample_throughput: 469.932\n",
            "    sample_time_ms: 8511.871\n",
            "    update_time_ms: 2.171\n",
            "  timestamp: 1650791785\n",
            "  timesteps_since_restore: 136000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 136000\n",
            "  training_iteration: 34\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:16:30 (running for 00:05:11.35)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         288.882</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">  192.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            192.39</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 140000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-16-34\n",
            "  done: false\n",
            "  episode_len_mean: 188.83\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 188.83\n",
            "  episode_reward_min: 22.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1176\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.8626451492309574e-10\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5609233637650808\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.001462210041786328\n",
            "          policy_loss: -0.0052856561337267195\n",
            "          total_loss: 9.491655298458632\n",
            "          vf_explained_var: 0.054971131970805505\n",
            "          vf_loss: 9.496940951193533\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 140000\n",
            "    num_agent_steps_trained: 140000\n",
            "    num_steps_sampled: 140000\n",
            "    num_steps_trained: 140000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 35\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.49166666666666\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11713955559934253\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14671508037022593\n",
            "    mean_inference_ms: 1.812180304613579\n",
            "    mean_raw_obs_processing_ms: 0.20141126343808388\n",
            "  time_since_restore: 297.42831778526306\n",
            "  time_this_iter_s: 8.546573638916016\n",
            "  time_total_s: 297.42831778526306\n",
            "  timers:\n",
            "    learn_throughput: 1041.953\n",
            "    learn_time_ms: 3838.944\n",
            "    load_throughput: 13448670.14\n",
            "    load_time_ms: 0.297\n",
            "    sample_throughput: 468.584\n",
            "    sample_time_ms: 8536.351\n",
            "    update_time_ms: 2.182\n",
            "  timestamp: 1650791794\n",
            "  timesteps_since_restore: 140000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 140000\n",
            "  training_iteration: 35\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:16:39 (running for 00:05:20.04)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         297.428</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  188.83</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            188.83</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 144000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-16-43\n",
            "  done: false\n",
            "  episode_len_mean: 187.54\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 187.54\n",
            "  episode_reward_min: 42.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1198\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.313225746154787e-11\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5658022277457739\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0006242887538240258\n",
            "          policy_loss: -0.003431901664182704\n",
            "          total_loss: 9.522793139180829\n",
            "          vf_explained_var: 0.056476282240242086\n",
            "          vf_loss: 9.526225042855868\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 144000\n",
            "    num_agent_steps_trained: 144000\n",
            "    num_steps_sampled: 144000\n",
            "    num_steps_trained: 144000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 36\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.76153846153845\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11719448075053783\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1467398281987674\n",
            "    mean_inference_ms: 1.8127172370559856\n",
            "    mean_raw_obs_processing_ms: 0.2013652500006403\n",
            "  time_since_restore: 305.9408769607544\n",
            "  time_this_iter_s: 8.512559175491333\n",
            "  time_total_s: 305.9408769607544\n",
            "  timers:\n",
            "    learn_throughput: 1042.169\n",
            "    learn_time_ms: 3838.15\n",
            "    load_throughput: 13262621.344\n",
            "    load_time_ms: 0.302\n",
            "    sample_throughput: 467.799\n",
            "    sample_time_ms: 8550.682\n",
            "    update_time_ms: 2.201\n",
            "  timestamp: 1650791803\n",
            "  timesteps_since_restore: 144000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 144000\n",
            "  training_iteration: 36\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:16:48 (running for 00:05:28.54)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         305.941</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">  187.54</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  42</td><td style=\"text-align: right;\">            187.54</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 148000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-16-51\n",
            "  done: false\n",
            "  episode_len_mean: 185.16\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 185.16\n",
            "  episode_reward_min: 42.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1220\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.6566128730773935e-11\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5555374029823529\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00035504934320858066\n",
            "          policy_loss: -0.009292597603553566\n",
            "          total_loss: 9.483041994033321\n",
            "          vf_explained_var: -0.005578195215553366\n",
            "          vf_loss: 9.492334602725121\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 148000\n",
            "    num_agent_steps_trained: 148000\n",
            "    num_steps_sampled: 148000\n",
            "    num_steps_trained: 148000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 37\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.48333333333332\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11724027876117407\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14672398987335086\n",
            "    mean_inference_ms: 1.812960599716407\n",
            "    mean_raw_obs_processing_ms: 0.201329449321891\n",
            "  time_since_restore: 314.4072651863098\n",
            "  time_this_iter_s: 8.46638822555542\n",
            "  time_total_s: 314.4072651863098\n",
            "  timers:\n",
            "    learn_throughput: 1039.827\n",
            "    learn_time_ms: 3846.792\n",
            "    load_throughput: 13218733.06\n",
            "    load_time_ms: 0.303\n",
            "    sample_throughput: 468.127\n",
            "    sample_time_ms: 8544.697\n",
            "    update_time_ms: 2.155\n",
            "  timestamp: 1650791811\n",
            "  timesteps_since_restore: 148000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 148000\n",
            "  training_iteration: 37\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:16:56 (running for 00:05:36.98)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         314.407</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  185.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  42</td><td style=\"text-align: right;\">            185.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 152000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-17-00\n",
            "  done: false\n",
            "  episode_len_mean: 186.46\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 186.46\n",
            "  episode_reward_min: 42.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1240\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.3283064365386967e-11\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.570781216057398\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.001072348294300299\n",
            "          policy_loss: -0.0034508399024445525\n",
            "          total_loss: 9.488938189065584\n",
            "          vf_explained_var: 0.0240933935488424\n",
            "          vf_loss: 9.492389025739444\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 152000\n",
            "    num_agent_steps_trained: 152000\n",
            "    num_steps_sampled: 152000\n",
            "    num_steps_trained: 152000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 38\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.175\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11729153727027691\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14678109102171125\n",
            "    mean_inference_ms: 1.8129555383372087\n",
            "    mean_raw_obs_processing_ms: 0.2011535674399498\n",
            "  time_since_restore: 322.8680245876312\n",
            "  time_this_iter_s: 8.460759401321411\n",
            "  time_total_s: 322.8680245876312\n",
            "  timers:\n",
            "    learn_throughput: 1039.339\n",
            "    learn_time_ms: 3848.602\n",
            "    load_throughput: 13463779.793\n",
            "    load_time_ms: 0.297\n",
            "    sample_throughput: 468.079\n",
            "    sample_time_ms: 8545.562\n",
            "    update_time_ms: 2.141\n",
            "  timestamp: 1650791820\n",
            "  timesteps_since_restore: 152000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 152000\n",
            "  training_iteration: 38\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:17:05 (running for 00:05:45.57)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         322.868</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">  186.46</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  42</td><td style=\"text-align: right;\">            186.46</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 156000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-17-08\n",
            "  done: false\n",
            "  episode_len_mean: 185.72\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 185.72\n",
            "  episode_reward_min: 27.0\n",
            "  episodes_this_iter: 23\n",
            "  episodes_total: 1263\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.1641532182693484e-11\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5677070617675781\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0013173030773421174\n",
            "          policy_loss: -0.002490641094583978\n",
            "          total_loss: 9.477876507338657\n",
            "          vf_explained_var: -0.12742194898666875\n",
            "          vf_loss: 9.480367120107015\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 156000\n",
            "    num_agent_steps_trained: 156000\n",
            "    num_steps_sampled: 156000\n",
            "    num_steps_trained: 156000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 39\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.35833333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1173424775764079\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14678504739028483\n",
            "    mean_inference_ms: 1.8128156340597184\n",
            "    mean_raw_obs_processing_ms: 0.20108538723653677\n",
            "  time_since_restore: 331.39600920677185\n",
            "  time_this_iter_s: 8.527984619140625\n",
            "  time_total_s: 331.39600920677185\n",
            "  timers:\n",
            "    learn_throughput: 1038.829\n",
            "    learn_time_ms: 3850.488\n",
            "    load_throughput: 13534378.832\n",
            "    load_time_ms: 0.296\n",
            "    sample_throughput: 467.797\n",
            "    sample_time_ms: 8550.725\n",
            "    update_time_ms: 2.151\n",
            "  timestamp: 1650791828\n",
            "  timesteps_since_restore: 156000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 156000\n",
            "  training_iteration: 39\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:17:13 (running for 00:05:54.11)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         331.396</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  185.72</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            185.72</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 160000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-17-17\n",
            "  done: false\n",
            "  episode_len_mean: 188.01\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 188.01\n",
            "  episode_reward_min: 27.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1283\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.820766091346742e-12\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5654508854753227\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 3.149049012599877e-05\n",
            "          policy_loss: -0.0014007880602793026\n",
            "          total_loss: 9.428747105342085\n",
            "          vf_explained_var: -0.1048680753477158\n",
            "          vf_loss: 9.430147892941712\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 160000\n",
            "    num_agent_steps_trained: 160000\n",
            "    num_steps_sampled: 160000\n",
            "    num_steps_trained: 160000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 40\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.71666666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11736510731252883\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14682883171168257\n",
            "    mean_inference_ms: 1.8127127314768785\n",
            "    mean_raw_obs_processing_ms: 0.20094511907377105\n",
            "  time_since_restore: 339.92888474464417\n",
            "  time_this_iter_s: 8.532875537872314\n",
            "  time_total_s: 339.92888474464417\n",
            "  timers:\n",
            "    learn_throughput: 1038.952\n",
            "    learn_time_ms: 3850.034\n",
            "    load_throughput: 13594697.35\n",
            "    load_time_ms: 0.294\n",
            "    sample_throughput: 467.263\n",
            "    sample_time_ms: 8560.497\n",
            "    update_time_ms: 2.131\n",
            "  timestamp: 1650791837\n",
            "  timesteps_since_restore: 160000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 160000\n",
            "  training_iteration: 40\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:17:22 (running for 00:06:02.61)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         339.929</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">  188.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            188.01</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 164000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-17-25\n",
            "  done: false\n",
            "  episode_len_mean: 189.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 189.25\n",
            "  episode_reward_min: 27.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1304\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.910383045673371e-12\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5689993535959592\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002951876988499978\n",
            "          policy_loss: -0.0031461782212699615\n",
            "          total_loss: 9.393226293338243\n",
            "          vf_explained_var: -0.05793058397949383\n",
            "          vf_loss: 9.396372469009892\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 164000\n",
            "    num_agent_steps_trained: 164000\n",
            "    num_steps_sampled: 164000\n",
            "    num_steps_trained: 164000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 41\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.21666666666668\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.117387084032623\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1468459359778041\n",
            "    mean_inference_ms: 1.81247669995633\n",
            "    mean_raw_obs_processing_ms: 0.2008746878857135\n",
            "  time_since_restore: 348.392347574234\n",
            "  time_this_iter_s: 8.463462829589844\n",
            "  time_total_s: 348.392347574234\n",
            "  timers:\n",
            "    learn_throughput: 1041.699\n",
            "    learn_time_ms: 3839.88\n",
            "    load_throughput: 13684515.498\n",
            "    load_time_ms: 0.292\n",
            "    sample_throughput: 467.268\n",
            "    sample_time_ms: 8560.405\n",
            "    update_time_ms: 2.234\n",
            "  timestamp: 1650791845\n",
            "  timesteps_since_restore: 164000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 164000\n",
            "  training_iteration: 41\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:17:30 (running for 00:06:11.20)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         348.392</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">  189.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            189.25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 168000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-17-34\n",
            "  done: false\n",
            "  episode_len_mean: 188.64\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 188.64\n",
            "  episode_reward_min: 27.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1325\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.4551915228366855e-12\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5558628489894252\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00044432568362390897\n",
            "          policy_loss: 0.0001707740438481172\n",
            "          total_loss: 9.478968351630755\n",
            "          vf_explained_var: -0.21728787928499202\n",
            "          vf_loss: 9.478797568044355\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 168000\n",
            "    num_agent_steps_trained: 168000\n",
            "    num_steps_sampled: 168000\n",
            "    num_steps_trained: 168000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 42\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.01666666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11742488798923041\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14690165324220075\n",
            "    mean_inference_ms: 1.8124410435140568\n",
            "    mean_raw_obs_processing_ms: 0.200756582350649\n",
            "  time_since_restore: 356.77157402038574\n",
            "  time_this_iter_s: 8.379226446151733\n",
            "  time_total_s: 356.77157402038574\n",
            "  timers:\n",
            "    learn_throughput: 1044.745\n",
            "    learn_time_ms: 3828.685\n",
            "    load_throughput: 13382161.602\n",
            "    load_time_ms: 0.299\n",
            "    sample_throughput: 468.006\n",
            "    sample_time_ms: 8546.899\n",
            "    update_time_ms: 2.252\n",
            "  timestamp: 1650791854\n",
            "  timesteps_since_restore: 168000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 168000\n",
            "  training_iteration: 42\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:17:39 (running for 00:06:19.57)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         356.772</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">  188.64</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            188.64</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 172000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-17-42\n",
            "  done: false\n",
            "  episode_len_mean: 191.67\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 191.67\n",
            "  episode_reward_min: 131.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1346\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.275957614183427e-13\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5646834116469147\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.001484836862967142\n",
            "          policy_loss: -0.0017234864174037851\n",
            "          total_loss: 9.429816459327617\n",
            "          vf_explained_var: -0.14129817287127178\n",
            "          vf_loss: 9.4315399354504\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 172000\n",
            "    num_agent_steps_trained: 172000\n",
            "    num_steps_sampled: 172000\n",
            "    num_steps_trained: 172000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 43\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.63846153846154\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1174752325647038\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1469119976595451\n",
            "    mean_inference_ms: 1.8125465225746518\n",
            "    mean_raw_obs_processing_ms: 0.20073008738485826\n",
            "  time_since_restore: 365.2716190814972\n",
            "  time_this_iter_s: 8.50004506111145\n",
            "  time_total_s: 365.2716190814972\n",
            "  timers:\n",
            "    learn_throughput: 1046.679\n",
            "    learn_time_ms: 3821.61\n",
            "    load_throughput: 13344906.141\n",
            "    load_time_ms: 0.3\n",
            "    sample_throughput: 468.632\n",
            "    sample_time_ms: 8535.474\n",
            "    update_time_ms: 2.177\n",
            "  timestamp: 1650791862\n",
            "  timesteps_since_restore: 172000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 172000\n",
            "  training_iteration: 43\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:17:47 (running for 00:06:28.06)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         365.272</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  191.67</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 131</td><td style=\"text-align: right;\">            191.67</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 176000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-17-51\n",
            "  done: false\n",
            "  episode_len_mean: 191.31\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 191.31\n",
            "  episode_reward_min: 127.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1368\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.6379788070917137e-13\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5524164936875784\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0004347559752113507\n",
            "          policy_loss: -0.006042644545756361\n",
            "          total_loss: 9.481778462727865\n",
            "          vf_explained_var: -0.16947289295093987\n",
            "          vf_loss: 9.487821132906022\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 176000\n",
            "    num_agent_steps_trained: 176000\n",
            "    num_steps_sampled: 176000\n",
            "    num_steps_trained: 176000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 44\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.33333333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11753095717978795\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14691979129093993\n",
            "    mean_inference_ms: 1.8125863225862866\n",
            "    mean_raw_obs_processing_ms: 0.20066343052045657\n",
            "  time_since_restore: 373.742951631546\n",
            "  time_this_iter_s: 8.471332550048828\n",
            "  time_total_s: 373.742951631546\n",
            "  timers:\n",
            "    learn_throughput: 1047.815\n",
            "    learn_time_ms: 3817.467\n",
            "    load_throughput: 13590292.426\n",
            "    load_time_ms: 0.294\n",
            "    sample_throughput: 469.195\n",
            "    sample_time_ms: 8525.244\n",
            "    update_time_ms: 2.128\n",
            "  timestamp: 1650791871\n",
            "  timesteps_since_restore: 176000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 176000\n",
            "  training_iteration: 44\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:17:56 (running for 00:06:36.64)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         373.743</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">  191.31</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            191.31</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 180000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-17-59\n",
            "  done: false\n",
            "  episode_len_mean: 192.16\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.16\n",
            "  episode_reward_min: 127.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1388\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.8189894035458568e-13\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.571206086489462\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00047820868277194964\n",
            "          policy_loss: -0.004297135241570011\n",
            "          total_loss: 9.350988490607149\n",
            "          vf_explained_var: -0.12084831569784431\n",
            "          vf_loss: 9.355285653760356\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 180000\n",
            "    num_agent_steps_trained: 180000\n",
            "    num_steps_sampled: 180000\n",
            "    num_steps_trained: 180000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 45\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.41666666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11756973263173713\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1469277237851009\n",
            "    mean_inference_ms: 1.8126746745755338\n",
            "    mean_raw_obs_processing_ms: 0.20058113845502087\n",
            "  time_since_restore: 382.2367169857025\n",
            "  time_this_iter_s: 8.493765354156494\n",
            "  time_total_s: 382.2367169857025\n",
            "  timers:\n",
            "    learn_throughput: 1047.906\n",
            "    learn_time_ms: 3817.135\n",
            "    load_throughput: 13416406.238\n",
            "    load_time_ms: 0.298\n",
            "    sample_throughput: 469.705\n",
            "    sample_time_ms: 8515.991\n",
            "    update_time_ms: 2.158\n",
            "  timestamp: 1650791879\n",
            "  timesteps_since_restore: 180000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 180000\n",
            "  training_iteration: 45\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:18:04 (running for 00:06:45.14)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         382.237</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  192.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            192.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 184000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-18-08\n",
            "  done: false\n",
            "  episode_len_mean: 191.85\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 191.85\n",
            "  episode_reward_min: 127.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1408\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 9.094947017729284e-14\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5719224590127187\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00035613183837561327\n",
            "          policy_loss: 0.0016725645470683293\n",
            "          total_loss: 9.250369630834108\n",
            "          vf_explained_var: -0.13478652155527504\n",
            "          vf_loss: 9.248697051181589\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 184000\n",
            "    num_agent_steps_trained: 184000\n",
            "    num_steps_sampled: 184000\n",
            "    num_steps_trained: 184000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 46\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.725\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11760571451398671\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14693021394702466\n",
            "    mean_inference_ms: 1.812866734855528\n",
            "    mean_raw_obs_processing_ms: 0.2005058176353164\n",
            "  time_since_restore: 390.7674322128296\n",
            "  time_this_iter_s: 8.530715227127075\n",
            "  time_total_s: 390.7674322128296\n",
            "  timers:\n",
            "    learn_throughput: 1047.449\n",
            "    learn_time_ms: 3818.802\n",
            "    load_throughput: 12873861.265\n",
            "    load_time_ms: 0.311\n",
            "    sample_throughput: 469.713\n",
            "    sample_time_ms: 8515.846\n",
            "    update_time_ms: 2.26\n",
            "  timestamp: 1650791888\n",
            "  timesteps_since_restore: 184000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 184000\n",
            "  training_iteration: 46\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:18:13 (running for 00:06:53.66)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         390.767</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">  191.85</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            191.85</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 188000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-18-16\n",
            "  done: false\n",
            "  episode_len_mean: 194.45\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 194.45\n",
            "  episode_reward_min: 127.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1428\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.547473508864642e-14\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5722068511670636\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00045525014773053677\n",
            "          policy_loss: 0.0010879357655843099\n",
            "          total_loss: 9.425004536618468\n",
            "          vf_explained_var: -0.17635956374547815\n",
            "          vf_loss: 9.423916596238332\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 188000\n",
            "    num_agent_steps_trained: 188000\n",
            "    num_steps_sampled: 188000\n",
            "    num_steps_trained: 188000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 47\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.49166666666666\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11764871689932636\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1469703194224447\n",
            "    mean_inference_ms: 1.8131872357061893\n",
            "    mean_raw_obs_processing_ms: 0.20038448992453875\n",
            "  time_since_restore: 399.3045701980591\n",
            "  time_this_iter_s: 8.537137985229492\n",
            "  time_total_s: 399.3045701980591\n",
            "  timers:\n",
            "    learn_throughput: 1047.954\n",
            "    learn_time_ms: 3816.96\n",
            "    load_throughput: 12783614.752\n",
            "    load_time_ms: 0.313\n",
            "    sample_throughput: 469.118\n",
            "    sample_time_ms: 8526.633\n",
            "    update_time_ms: 2.465\n",
            "  timestamp: 1650791896\n",
            "  timesteps_since_restore: 188000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 188000\n",
            "  training_iteration: 47\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:18:21 (running for 00:07:02.32)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         399.305</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  194.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            194.45</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 192000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-18-25\n",
            "  done: false\n",
            "  episode_len_mean: 193.73\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 193.73\n",
            "  episode_reward_min: 127.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1449\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.273736754432321e-14\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.578575655465485\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0025631012530251502\n",
            "          policy_loss: -0.005469098724725265\n",
            "          total_loss: 9.585345767646707\n",
            "          vf_explained_var: -0.2471023113496842\n",
            "          vf_loss: 9.590814879632765\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 192000\n",
            "    num_agent_steps_trained: 192000\n",
            "    num_steps_sampled: 192000\n",
            "    num_steps_trained: 192000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 48\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.7923076923077\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11765370999916741\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14696234714409187\n",
            "    mean_inference_ms: 1.8133924097055762\n",
            "    mean_raw_obs_processing_ms: 0.20038449744961198\n",
            "  time_since_restore: 407.85783648490906\n",
            "  time_this_iter_s: 8.553266286849976\n",
            "  time_total_s: 407.85783648490906\n",
            "  timers:\n",
            "    learn_throughput: 1049.407\n",
            "    learn_time_ms: 3811.676\n",
            "    load_throughput: 12753489.928\n",
            "    load_time_ms: 0.314\n",
            "    sample_throughput: 468.415\n",
            "    sample_time_ms: 8539.433\n",
            "    update_time_ms: 2.45\n",
            "  timestamp: 1650791905\n",
            "  timesteps_since_restore: 192000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 192000\n",
            "  training_iteration: 48\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:18:30 (running for 00:07:10.88)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         407.858</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">  193.73</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 127</td><td style=\"text-align: right;\">            193.73</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 196000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-18-33\n",
            "  done: false\n",
            "  episode_len_mean: 192.67\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.67\n",
            "  episode_reward_min: 139.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1471\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.1368683772161605e-14\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5793785535520123\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0006892615349706058\n",
            "          policy_loss: 0.0012484980296463735\n",
            "          total_loss: 9.354372935654014\n",
            "          vf_explained_var: -0.28513176146373953\n",
            "          vf_loss: 9.353124482144592\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 196000\n",
            "    num_agent_steps_trained: 196000\n",
            "    num_steps_sampled: 196000\n",
            "    num_steps_trained: 196000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 49\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.68333333333332\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11766320636584265\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14698443826849414\n",
            "    mean_inference_ms: 1.8138034530779226\n",
            "    mean_raw_obs_processing_ms: 0.20035924777220962\n",
            "  time_since_restore: 416.38401675224304\n",
            "  time_this_iter_s: 8.526180267333984\n",
            "  time_total_s: 416.38401675224304\n",
            "  timers:\n",
            "    learn_throughput: 1049.26\n",
            "    learn_time_ms: 3812.212\n",
            "    load_throughput: 10321264.842\n",
            "    load_time_ms: 0.388\n",
            "    sample_throughput: 468.712\n",
            "    sample_time_ms: 8534.028\n",
            "    update_time_ms: 2.507\n",
            "  timestamp: 1650791913\n",
            "  timesteps_since_restore: 196000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 196000\n",
            "  training_iteration: 49\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:18:39 (running for 00:07:19.38)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         416.384</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  192.67</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 139</td><td style=\"text-align: right;\">            192.67</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 200000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-18-42\n",
            "  done: false\n",
            "  episode_len_mean: 189.01\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 189.01\n",
            "  episode_reward_min: 98.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1493\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.6843418860808026e-15\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5751144515570774\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0005385970526518931\n",
            "          policy_loss: -0.004146970231686869\n",
            "          total_loss: 9.379499357490129\n",
            "          vf_explained_var: -0.2730767379524887\n",
            "          vf_loss: 9.383646313862133\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 200000\n",
            "    num_agent_steps_trained: 200000\n",
            "    num_steps_sampled: 200000\n",
            "    num_steps_trained: 200000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 50\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.075\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11768583855966977\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14697012604813237\n",
            "    mean_inference_ms: 1.8140198660523879\n",
            "    mean_raw_obs_processing_ms: 0.2003840778288557\n",
            "  time_since_restore: 424.8879396915436\n",
            "  time_this_iter_s: 8.503922939300537\n",
            "  time_total_s: 424.8879396915436\n",
            "  timers:\n",
            "    learn_throughput: 1049.792\n",
            "    learn_time_ms: 3810.28\n",
            "    load_throughput: 10380013.611\n",
            "    load_time_ms: 0.385\n",
            "    sample_throughput: 468.814\n",
            "    sample_time_ms: 8532.172\n",
            "    update_time_ms: 2.523\n",
            "  timestamp: 1650791922\n",
            "  timesteps_since_restore: 200000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 200000\n",
            "  training_iteration: 50\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:18:47 (running for 00:07:28.00)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         424.888</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">  189.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  98</td><td style=\"text-align: right;\">            189.01</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 204000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-18-51\n",
            "  done: false\n",
            "  episode_len_mean: 186.6\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 186.6\n",
            "  episode_reward_min: 98.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1515\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.8421709430404013e-15\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.577980641780361\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0030553529398679134\n",
            "          policy_loss: -0.007904138968836877\n",
            "          total_loss: 9.452188515406784\n",
            "          vf_explained_var: -0.2579213676914092\n",
            "          vf_loss: 9.460092637872183\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 204000\n",
            "    num_agent_steps_trained: 204000\n",
            "    num_steps_sampled: 204000\n",
            "    num_steps_trained: 204000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 51\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.43333333333332\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11769926977675765\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14694974685001072\n",
            "    mean_inference_ms: 1.8141655551672158\n",
            "    mean_raw_obs_processing_ms: 0.20040677104035398\n",
            "  time_since_restore: 433.4084565639496\n",
            "  time_this_iter_s: 8.520516872406006\n",
            "  time_total_s: 433.4084565639496\n",
            "  timers:\n",
            "    learn_throughput: 1048.125\n",
            "    learn_time_ms: 3816.34\n",
            "    load_throughput: 10325076.005\n",
            "    load_time_ms: 0.387\n",
            "    sample_throughput: 468.955\n",
            "    sample_time_ms: 8529.601\n",
            "    update_time_ms: 2.405\n",
            "  timestamp: 1650791931\n",
            "  timesteps_since_restore: 204000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 204000\n",
            "  training_iteration: 51\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:18:56 (running for 00:07:36.53)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         433.408</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">   186.6</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  98</td><td style=\"text-align: right;\">             186.6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 208000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-18-59\n",
            "  done: false\n",
            "  episode_len_mean: 179.35\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 179.35\n",
            "  episode_reward_min: 98.0\n",
            "  episodes_this_iter: 24\n",
            "  episodes_total: 1539\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.4210854715202006e-15\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5762204343272793\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0028698095941400406\n",
            "          policy_loss: 0.0017025529487078549\n",
            "          total_loss: 9.281833611765215\n",
            "          vf_explained_var: -0.31574045676057055\n",
            "          vf_loss: 9.280131033415435\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 208000\n",
            "    num_agent_steps_trained: 208000\n",
            "    num_steps_sampled: 208000\n",
            "    num_steps_trained: 208000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 52\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.85833333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11770502846796849\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14691409693249774\n",
            "    mean_inference_ms: 1.8141833303489339\n",
            "    mean_raw_obs_processing_ms: 0.20042016942222987\n",
            "  time_since_restore: 441.9521985054016\n",
            "  time_this_iter_s: 8.543741941452026\n",
            "  time_total_s: 441.9521985054016\n",
            "  timers:\n",
            "    learn_throughput: 1043.4\n",
            "    learn_time_ms: 3833.621\n",
            "    load_throughput: 10467441.977\n",
            "    load_time_ms: 0.382\n",
            "    sample_throughput: 468.637\n",
            "    sample_time_ms: 8535.399\n",
            "    update_time_ms: 2.368\n",
            "  timestamp: 1650791939\n",
            "  timesteps_since_restore: 208000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 208000\n",
            "  training_iteration: 52\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:19:04 (running for 00:07:45.05)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         441.952</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">  179.35</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  98</td><td style=\"text-align: right;\">            179.35</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 212000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-19-08\n",
            "  done: false\n",
            "  episode_len_mean: 177.97\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 177.97\n",
            "  episode_reward_min: 98.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1561\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 7.105427357601003e-16\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5734012847305626\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00320931537244541\n",
            "          policy_loss: -0.0011741512825572363\n",
            "          total_loss: 9.21807260667124\n",
            "          vf_explained_var: -0.2954997319047169\n",
            "          vf_loss: 9.219246733573176\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 212000\n",
            "    num_agent_steps_trained: 212000\n",
            "    num_steps_sampled: 212000\n",
            "    num_steps_trained: 212000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 53\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.89166666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11773131066956923\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14689966346006514\n",
            "    mean_inference_ms: 1.8139937066441387\n",
            "    mean_raw_obs_processing_ms: 0.2003616352180218\n",
            "  time_since_restore: 450.35281705856323\n",
            "  time_this_iter_s: 8.400618553161621\n",
            "  time_total_s: 450.35281705856323\n",
            "  timers:\n",
            "    learn_throughput: 1042.247\n",
            "    learn_time_ms: 3837.86\n",
            "    load_throughput: 10491661.56\n",
            "    load_time_ms: 0.381\n",
            "    sample_throughput: 468.425\n",
            "    sample_time_ms: 8539.248\n",
            "    update_time_ms: 2.42\n",
            "  timestamp: 1650791948\n",
            "  timesteps_since_restore: 212000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 212000\n",
            "  training_iteration: 53\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:19:13 (running for 00:07:53.57)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         450.353</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  177.97</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  98</td><td style=\"text-align: right;\">            177.97</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 216000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-19-16\n",
            "  done: false\n",
            "  episode_len_mean: 180.08\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 180.08\n",
            "  episode_reward_min: 121.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1582\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.5527136788005016e-16\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5761338897289768\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0031979973077282415\n",
            "          policy_loss: -0.0020300497255858873\n",
            "          total_loss: 9.423355493237896\n",
            "          vf_explained_var: -0.17377544199266742\n",
            "          vf_loss: 9.425385564373386\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 216000\n",
            "    num_agent_steps_trained: 216000\n",
            "    num_steps_sampled: 216000\n",
            "    num_steps_trained: 216000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 54\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.54615384615384\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11774511190065252\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14685921304993305\n",
            "    mean_inference_ms: 1.8137796113432316\n",
            "    mean_raw_obs_processing_ms: 0.20039052030639598\n",
            "  time_since_restore: 458.91958594322205\n",
            "  time_this_iter_s: 8.566768884658813\n",
            "  time_total_s: 458.91958594322205\n",
            "  timers:\n",
            "    learn_throughput: 1041.241\n",
            "    learn_time_ms: 3841.569\n",
            "    load_throughput: 10314285.012\n",
            "    load_time_ms: 0.388\n",
            "    sample_throughput: 467.921\n",
            "    sample_time_ms: 8548.457\n",
            "    update_time_ms: 2.47\n",
            "  timestamp: 1650791956\n",
            "  timesteps_since_restore: 216000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 216000\n",
            "  training_iteration: 54\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:19:21 (running for 00:08:02.14)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          458.92</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">  180.08</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 121</td><td style=\"text-align: right;\">            180.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 220000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-19-25\n",
            "  done: false\n",
            "  episode_len_mean: 180.3\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 180.3\n",
            "  episode_reward_min: 73.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1603\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.7763568394002508e-16\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5779854436715444\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0005758779365542789\n",
            "          policy_loss: 0.0024359838135780826\n",
            "          total_loss: 9.316075578299902\n",
            "          vf_explained_var: -0.38024312027039064\n",
            "          vf_loss: 9.313639599277128\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 220000\n",
            "    num_agent_steps_trained: 220000\n",
            "    num_steps_sampled: 220000\n",
            "    num_steps_trained: 220000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 55\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.56666666666666\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11775111742515869\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1468717673579051\n",
            "    mean_inference_ms: 1.813750650756883\n",
            "    mean_raw_obs_processing_ms: 0.2003269142093956\n",
            "  time_since_restore: 467.3973867893219\n",
            "  time_this_iter_s: 8.477800846099854\n",
            "  time_total_s: 467.3973867893219\n",
            "  timers:\n",
            "    learn_throughput: 1041.33\n",
            "    learn_time_ms: 3841.241\n",
            "    load_throughput: 10348002.22\n",
            "    load_time_ms: 0.387\n",
            "    sample_throughput: 467.757\n",
            "    sample_time_ms: 8551.446\n",
            "    update_time_ms: 2.447\n",
            "  timestamp: 1650791965\n",
            "  timesteps_since_restore: 220000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 220000\n",
            "  training_iteration: 55\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:19:30 (running for 00:08:10.61)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         467.397</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">   180.3</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">             180.3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 224000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-19-33\n",
            "  done: false\n",
            "  episode_len_mean: 182.4\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 182.4\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1624\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 8.881784197001254e-17\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5783438412092066\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00023320565779565643\n",
            "          policy_loss: 0.003085373680517879\n",
            "          total_loss: 9.38633152490021\n",
            "          vf_explained_var: -0.4707480283834601\n",
            "          vf_loss: 9.383246146991688\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 224000\n",
            "    num_agent_steps_trained: 224000\n",
            "    num_steps_sampled: 224000\n",
            "    num_steps_trained: 224000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 56\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.46666666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11776448799082093\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14685918695587583\n",
            "    mean_inference_ms: 1.8137370945323334\n",
            "    mean_raw_obs_processing_ms: 0.2003198863846589\n",
            "  time_since_restore: 475.95584177970886\n",
            "  time_this_iter_s: 8.558454990386963\n",
            "  time_total_s: 475.95584177970886\n",
            "  timers:\n",
            "    learn_throughput: 1040.9\n",
            "    learn_time_ms: 3842.829\n",
            "    load_throughput: 10688848.114\n",
            "    load_time_ms: 0.374\n",
            "    sample_throughput: 467.683\n",
            "    sample_time_ms: 8552.8\n",
            "    update_time_ms: 2.308\n",
            "  timestamp: 1650791973\n",
            "  timesteps_since_restore: 224000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 224000\n",
            "  training_iteration: 56\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:19:38 (running for 00:08:19.29)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         475.956</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">   182.4</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">             182.4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 228000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-19-42\n",
            "  done: false\n",
            "  episode_len_mean: 187.24\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 187.24\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1646\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.440892098500627e-17\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5751623835614933\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0028390353274866604\n",
            "          policy_loss: 8.045964303516572e-05\n",
            "          total_loss: 9.168579373821135\n",
            "          vf_explained_var: -0.5618781067991769\n",
            "          vf_loss: 9.168498912421606\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 228000\n",
            "    num_agent_steps_trained: 228000\n",
            "    num_steps_sampled: 228000\n",
            "    num_steps_trained: 228000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 57\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.26666666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11777705431428542\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1468841813269559\n",
            "    mean_inference_ms: 1.8139196822892976\n",
            "    mean_raw_obs_processing_ms: 0.2002687558071792\n",
            "  time_since_restore: 484.4237253665924\n",
            "  time_this_iter_s: 8.467883586883545\n",
            "  time_total_s: 484.4237253665924\n",
            "  timers:\n",
            "    learn_throughput: 1041.848\n",
            "    learn_time_ms: 3839.332\n",
            "    load_throughput: 10581656.26\n",
            "    load_time_ms: 0.378\n",
            "    sample_throughput: 467.77\n",
            "    sample_time_ms: 8551.22\n",
            "    update_time_ms: 2.079\n",
            "  timestamp: 1650791982\n",
            "  timesteps_since_restore: 228000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 228000\n",
            "  training_iteration: 57\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:19:47 (running for 00:08:27.76)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         484.424</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  187.24</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            187.24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 232000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-19-50\n",
            "  done: false\n",
            "  episode_len_mean: 188.65\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 188.65\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 1667\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.2204460492503135e-17\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5861041665077209\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0030544035560643233\n",
            "          policy_loss: 0.002638512377136497\n",
            "          total_loss: 9.180256869203301\n",
            "          vf_explained_var: -0.4779194654316031\n",
            "          vf_loss: 9.177618332319362\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 232000\n",
            "    num_agent_steps_trained: 232000\n",
            "    num_steps_sampled: 232000\n",
            "    num_steps_trained: 232000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 58\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.54166666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11778292131244174\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14689449491276446\n",
            "    mean_inference_ms: 1.814084841230728\n",
            "    mean_raw_obs_processing_ms: 0.2002800516431806\n",
            "  time_since_restore: 492.9062762260437\n",
            "  time_this_iter_s: 8.482550859451294\n",
            "  time_total_s: 492.9062762260437\n",
            "  timers:\n",
            "    learn_throughput: 1042.469\n",
            "    learn_time_ms: 3837.045\n",
            "    load_throughput: 10626561.946\n",
            "    load_time_ms: 0.376\n",
            "    sample_throughput: 468.249\n",
            "    sample_time_ms: 8542.47\n",
            "    update_time_ms: 2.077\n",
            "  timestamp: 1650791990\n",
            "  timesteps_since_restore: 232000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 232000\n",
            "  training_iteration: 58\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:19:55 (running for 00:08:36.23)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         492.906</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">  188.65</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            188.65</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 236000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-19-59\n",
            "  done: false\n",
            "  episode_len_mean: 185.99\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 185.99\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 23\n",
            "  episodes_total: 1690\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.1102230246251568e-17\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5863507937359553\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003426102894368321\n",
            "          policy_loss: 0.005568124538147322\n",
            "          total_loss: 9.021968883596442\n",
            "          vf_explained_var: -0.558405287099141\n",
            "          vf_loss: 9.016400734583536\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 236000\n",
            "    num_agent_steps_trained: 236000\n",
            "    num_steps_sampled: 236000\n",
            "    num_steps_trained: 236000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 59\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.25833333333334\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11778393782302579\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1469052950484955\n",
            "    mean_inference_ms: 1.8142272912393134\n",
            "    mean_raw_obs_processing_ms: 0.2002579508759458\n",
            "  time_since_restore: 501.42154908180237\n",
            "  time_this_iter_s: 8.515272855758667\n",
            "  time_total_s: 501.42154908180237\n",
            "  timers:\n",
            "    learn_throughput: 1041.832\n",
            "    learn_time_ms: 3839.39\n",
            "    load_throughput: 13105152.32\n",
            "    load_time_ms: 0.305\n",
            "    sample_throughput: 468.551\n",
            "    sample_time_ms: 8536.957\n",
            "    update_time_ms: 2.017\n",
            "  timestamp: 1650791999\n",
            "  timesteps_since_restore: 236000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 236000\n",
            "  training_iteration: 59\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:20:04 (running for 00:08:44.86)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         501.422</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  185.99</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            185.99</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 240000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-20-07\n",
            "  done: false\n",
            "  episode_len_mean: 181.07\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 181.07\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 23\n",
            "  episodes_total: 1713\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.551115123125784e-18\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5768563134054984\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.001212835624017105\n",
            "          policy_loss: -0.008208702401488379\n",
            "          total_loss: 9.215693051840669\n",
            "          vf_explained_var: -0.5698948378844928\n",
            "          vf_loss: 9.223901762500885\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 240000\n",
            "    num_agent_steps_trained: 240000\n",
            "    num_steps_sampled: 240000\n",
            "    num_steps_trained: 240000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 60\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.72307692307692\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11779502900002631\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14686014150685256\n",
            "    mean_inference_ms: 1.8143390960991923\n",
            "    mean_raw_obs_processing_ms: 0.20033978027942823\n",
            "  time_since_restore: 510.0047810077667\n",
            "  time_this_iter_s: 8.583231925964355\n",
            "  time_total_s: 510.0047810077667\n",
            "  timers:\n",
            "    learn_throughput: 1042.388\n",
            "    learn_time_ms: 3837.341\n",
            "    load_throughput: 13084710.654\n",
            "    load_time_ms: 0.306\n",
            "    sample_throughput: 467.898\n",
            "    sample_time_ms: 8548.874\n",
            "    update_time_ms: 1.996\n",
            "  timestamp: 1650792007\n",
            "  timesteps_since_restore: 240000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 240000\n",
            "  training_iteration: 60\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:20:13 (running for 00:08:53.45)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         510.005</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">  181.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            181.07</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 244000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-20-16\n",
            "  done: false\n",
            "  episode_len_mean: 173.14\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 173.14\n",
            "  episode_reward_min: 40.0\n",
            "  episodes_this_iter: 26\n",
            "  episodes_total: 1739\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.775557561562892e-18\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5765879242010014\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002554927044133164\n",
            "          policy_loss: 0.001292708299813732\n",
            "          total_loss: 9.10390821528691\n",
            "          vf_explained_var: -0.6374127469396078\n",
            "          vf_loss: 9.102615517954673\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 244000\n",
            "    num_agent_steps_trained: 244000\n",
            "    num_steps_sampled: 244000\n",
            "    num_steps_trained: 244000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 61\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.25833333333334\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11776982269116759\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.146937026691845\n",
            "    mean_inference_ms: 1.8146075161171302\n",
            "    mean_raw_obs_processing_ms: 0.20021094654228086\n",
            "  time_since_restore: 518.4863164424896\n",
            "  time_this_iter_s: 8.4815354347229\n",
            "  time_total_s: 518.4863164424896\n",
            "  timers:\n",
            "    learn_throughput: 1042.389\n",
            "    learn_time_ms: 3837.338\n",
            "    load_throughput: 13237506.707\n",
            "    load_time_ms: 0.302\n",
            "    sample_throughput: 468.187\n",
            "    sample_time_ms: 8543.603\n",
            "    update_time_ms: 2.035\n",
            "  timestamp: 1650792016\n",
            "  timesteps_since_restore: 244000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 244000\n",
            "  training_iteration: 61\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:20:21 (running for 00:09:01.90)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         518.486</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  173.14</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  40</td><td style=\"text-align: right;\">            173.14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 248000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-20-24\n",
            "  done: false\n",
            "  episode_len_mean: 170.07\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 170.07\n",
            "  episode_reward_min: 40.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1761\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.387778780781446e-18\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5841460552907759\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0018236408033486865\n",
            "          policy_loss: 0.003939533714325197\n",
            "          total_loss: 8.984915590798982\n",
            "          vf_explained_var: -0.5282676070608119\n",
            "          vf_loss: 8.980976045772593\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 248000\n",
            "    num_agent_steps_trained: 248000\n",
            "    num_steps_sampled: 248000\n",
            "    num_steps_trained: 248000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 62\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.60833333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11772672954195076\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14695565977394606\n",
            "    mean_inference_ms: 1.8145195274871821\n",
            "    mean_raw_obs_processing_ms: 0.200140335368855\n",
            "  time_since_restore: 526.8191199302673\n",
            "  time_this_iter_s: 8.33280348777771\n",
            "  time_total_s: 526.8191199302673\n",
            "  timers:\n",
            "    learn_throughput: 1044.899\n",
            "    learn_time_ms: 3828.121\n",
            "    load_throughput: 13385364.608\n",
            "    load_time_ms: 0.299\n",
            "    sample_throughput: 468.877\n",
            "    sample_time_ms: 8531.021\n",
            "    update_time_ms: 2.039\n",
            "  timestamp: 1650792024\n",
            "  timesteps_since_restore: 248000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 248000\n",
            "  training_iteration: 62\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:20:29 (running for 00:09:10.36)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         526.819</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">  170.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  40</td><td style=\"text-align: right;\">            170.07</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 252000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-20-33\n",
            "  done: false\n",
            "  episode_len_mean: 162.98\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 162.98\n",
            "  episode_reward_min: 100.0\n",
            "  episodes_this_iter: 26\n",
            "  episodes_total: 1787\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 6.93889390390723e-19\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5833410634148506\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002125195291586068\n",
            "          policy_loss: 0.005782305345099459\n",
            "          total_loss: 9.170904133909492\n",
            "          vf_explained_var: -0.6178305671420149\n",
            "          vf_loss: 9.165121810666976\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 252000\n",
            "    num_agent_steps_trained: 252000\n",
            "    num_steps_sampled: 252000\n",
            "    num_steps_trained: 252000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 63\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.45\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11769543035546878\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1469647733714316\n",
            "    mean_inference_ms: 1.8143885188192825\n",
            "    mean_raw_obs_processing_ms: 0.20009447255299773\n",
            "  time_since_restore: 535.354926109314\n",
            "  time_this_iter_s: 8.53580617904663\n",
            "  time_total_s: 535.354926109314\n",
            "  timers:\n",
            "    learn_throughput: 1042.854\n",
            "    learn_time_ms: 3835.628\n",
            "    load_throughput: 13333240.086\n",
            "    load_time_ms: 0.3\n",
            "    sample_throughput: 469.044\n",
            "    sample_time_ms: 8527.987\n",
            "    update_time_ms: 2.026\n",
            "  timestamp: 1650792033\n",
            "  timesteps_since_restore: 252000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 252000\n",
            "  training_iteration: 63\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:20:38 (running for 00:09:18.90)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         535.355</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  162.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">            162.98</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 256000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-20-41\n",
            "  done: false\n",
            "  episode_len_mean: 161.44\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 161.44\n",
            "  episode_reward_min: 100.0\n",
            "  episodes_this_iter: 25\n",
            "  episodes_total: 1812\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.469446951953615e-19\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5795802759867843\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0008766694211577043\n",
            "          policy_loss: 0.007054938381958392\n",
            "          total_loss: 9.1859718215081\n",
            "          vf_explained_var: -0.6726752779817069\n",
            "          vf_loss: 9.178916850654028\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 256000\n",
            "    num_agent_steps_trained: 256000\n",
            "    num_steps_sampled: 256000\n",
            "    num_steps_trained: 256000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 64\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.425\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11769009481355144\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14698914246192266\n",
            "    mean_inference_ms: 1.8141724286580854\n",
            "    mean_raw_obs_processing_ms: 0.19999911515896263\n",
            "  time_since_restore: 543.8205876350403\n",
            "  time_this_iter_s: 8.465661525726318\n",
            "  time_total_s: 543.8205876350403\n",
            "  timers:\n",
            "    learn_throughput: 1044.197\n",
            "    learn_time_ms: 3830.694\n",
            "    load_throughput: 13604618.878\n",
            "    load_time_ms: 0.294\n",
            "    sample_throughput: 468.921\n",
            "    sample_time_ms: 8530.228\n",
            "    update_time_ms: 2.06\n",
            "  timestamp: 1650792041\n",
            "  timesteps_since_restore: 256000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 256000\n",
            "  training_iteration: 64\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:20:46 (running for 00:09:27.35)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         543.821</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  161.44</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">            161.44</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 260000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-20-50\n",
            "  done: false\n",
            "  episode_len_mean: 166.7\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 166.7\n",
            "  episode_reward_min: 85.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1834\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.7347234759768074e-19\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5896087288215596\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0002552454950774454\n",
            "          policy_loss: 0.00563842246949833\n",
            "          total_loss: 8.556963749854795\n",
            "          vf_explained_var: -0.423495287728566\n",
            "          vf_loss: 8.55132532376115\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 260000\n",
            "    num_agent_steps_trained: 260000\n",
            "    num_steps_sampled: 260000\n",
            "    num_steps_trained: 260000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 65\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.59166666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11771963315036521\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1468921124212647\n",
            "    mean_inference_ms: 1.8137599498563564\n",
            "    mean_raw_obs_processing_ms: 0.20011013015531864\n",
            "  time_since_restore: 552.3284914493561\n",
            "  time_this_iter_s: 8.507903814315796\n",
            "  time_total_s: 552.3284914493561\n",
            "  timers:\n",
            "    learn_throughput: 1043.818\n",
            "    learn_time_ms: 3832.087\n",
            "    load_throughput: 13661115.544\n",
            "    load_time_ms: 0.293\n",
            "    sample_throughput: 469.119\n",
            "    sample_time_ms: 8526.615\n",
            "    update_time_ms: 2.042\n",
            "  timestamp: 1650792050\n",
            "  timesteps_since_restore: 260000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 260000\n",
            "  training_iteration: 65\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:20:55 (running for 00:09:35.97)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         552.328</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">   166.7</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  85</td><td style=\"text-align: right;\">             166.7</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 264000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-20-59\n",
            "  done: false\n",
            "  episode_len_mean: 164.52\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 164.52\n",
            "  episode_reward_min: 64.0\n",
            "  episodes_this_iter: 25\n",
            "  episodes_total: 1859\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 8.673617379884037e-20\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5832664574987145\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0003662909070381057\n",
            "          policy_loss: 0.004230213741100924\n",
            "          total_loss: 8.551849018630161\n",
            "          vf_explained_var: -0.4954159342473553\n",
            "          vf_loss: 8.547618835203108\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 264000\n",
            "    num_agent_steps_trained: 264000\n",
            "    num_steps_sampled: 264000\n",
            "    num_steps_trained: 264000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 66\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.4\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11774373826156882\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1468938381455733\n",
            "    mean_inference_ms: 1.813788552249904\n",
            "    mean_raw_obs_processing_ms: 0.20004684316010837\n",
            "  time_since_restore: 560.842086315155\n",
            "  time_this_iter_s: 8.51359486579895\n",
            "  time_total_s: 560.842086315155\n",
            "  timers:\n",
            "    learn_throughput: 1043.457\n",
            "    learn_time_ms: 3833.412\n",
            "    load_throughput: 13749562.367\n",
            "    load_time_ms: 0.291\n",
            "    sample_throughput: 469.394\n",
            "    sample_time_ms: 8521.617\n",
            "    update_time_ms: 2.06\n",
            "  timestamp: 1650792059\n",
            "  timesteps_since_restore: 264000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 264000\n",
            "  training_iteration: 66\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:21:04 (running for 00:09:44.49)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         560.842</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">  164.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            164.52</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 268000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-21-07\n",
            "  done: false\n",
            "  episode_len_mean: 170.07\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 170.07\n",
            "  episode_reward_min: 64.0\n",
            "  episodes_this_iter: 23\n",
            "  episodes_total: 1882\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.3368086899420186e-20\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.582492329741037\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00035804752781384917\n",
            "          policy_loss: 0.007500285398896023\n",
            "          total_loss: 8.74727355023866\n",
            "          vf_explained_var: -0.5866491427985571\n",
            "          vf_loss: 8.739773249882523\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 268000\n",
            "    num_agent_steps_trained: 268000\n",
            "    num_steps_sampled: 268000\n",
            "    num_steps_trained: 268000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 67\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.425\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11776101923773265\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14683691051175451\n",
            "    mean_inference_ms: 1.8136809143380759\n",
            "    mean_raw_obs_processing_ms: 0.20006452926105284\n",
            "  time_since_restore: 569.2970232963562\n",
            "  time_this_iter_s: 8.454936981201172\n",
            "  time_total_s: 569.2970232963562\n",
            "  timers:\n",
            "    learn_throughput: 1042.62\n",
            "    learn_time_ms: 3836.488\n",
            "    load_throughput: 14160378.123\n",
            "    load_time_ms: 0.282\n",
            "    sample_throughput: 469.589\n",
            "    sample_time_ms: 8518.095\n",
            "    update_time_ms: 2.1\n",
            "  timestamp: 1650792067\n",
            "  timesteps_since_restore: 268000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 268000\n",
            "  training_iteration: 67\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:21:12 (running for 00:09:52.92)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         569.297</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  170.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            170.07</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 272000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-21-16\n",
            "  done: false\n",
            "  episode_len_mean: 170.42\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 170.42\n",
            "  episode_reward_min: 64.0\n",
            "  episodes_this_iter: 24\n",
            "  episodes_total: 1906\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.1684043449710093e-20\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.589167522102274\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004339234827223342\n",
            "          policy_loss: 0.002880404486511183\n",
            "          total_loss: 8.768358557711366\n",
            "          vf_explained_var: -0.42745801332176375\n",
            "          vf_loss: 8.765478142871652\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 272000\n",
            "    num_agent_steps_trained: 272000\n",
            "    num_steps_sampled: 272000\n",
            "    num_steps_trained: 272000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 68\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.4\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11776933320707116\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14679038159922736\n",
            "    mean_inference_ms: 1.8136088899014582\n",
            "    mean_raw_obs_processing_ms: 0.2000763330397157\n",
            "  time_since_restore: 577.8220071792603\n",
            "  time_this_iter_s: 8.524983882904053\n",
            "  time_total_s: 577.8220071792603\n",
            "  timers:\n",
            "    learn_throughput: 1041.092\n",
            "    learn_time_ms: 3842.118\n",
            "    load_throughput: 14271194.284\n",
            "    load_time_ms: 0.28\n",
            "    sample_throughput: 469.45\n",
            "    sample_time_ms: 8520.61\n",
            "    update_time_ms: 2.133\n",
            "  timestamp: 1650792076\n",
            "  timesteps_since_restore: 272000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 272000\n",
            "  training_iteration: 68\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:21:21 (running for 00:10:01.62)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         577.822</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">  170.42</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            170.42</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 276000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-21-25\n",
            "  done: false\n",
            "  episode_len_mean: 168.18\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 168.18\n",
            "  episode_reward_min: 64.0\n",
            "  episodes_this_iter: 24\n",
            "  episodes_total: 1930\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.0842021724855046e-20\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5872558982141556\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00026275159880303144\n",
            "          policy_loss: 0.008031877092215964\n",
            "          total_loss: 8.505886462426954\n",
            "          vf_explained_var: -0.6362809618955018\n",
            "          vf_loss: 8.497854576572296\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 276000\n",
            "    num_agent_steps_trained: 276000\n",
            "    num_steps_sampled: 276000\n",
            "    num_steps_trained: 276000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 69\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.025\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11775730878479322\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14682233851965984\n",
            "    mean_inference_ms: 1.81447786309943\n",
            "    mean_raw_obs_processing_ms: 0.2000161463072267\n",
            "  time_since_restore: 586.7783904075623\n",
            "  time_this_iter_s: 8.956383228302002\n",
            "  time_total_s: 586.7783904075623\n",
            "  timers:\n",
            "    learn_throughput: 1042.534\n",
            "    learn_time_ms: 3836.806\n",
            "    load_throughput: 11430178.498\n",
            "    load_time_ms: 0.35\n",
            "    sample_throughput: 466.451\n",
            "    sample_time_ms: 8575.4\n",
            "    update_time_ms: 2.145\n",
            "  timestamp: 1650792085\n",
            "  timesteps_since_restore: 276000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 276000\n",
            "  training_iteration: 69\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:21:30 (running for 00:10:10.53)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         586.778</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  168.18</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            168.18</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 280000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-21-33\n",
            "  done: false\n",
            "  episode_len_mean: 165.94\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 165.94\n",
            "  episode_reward_min: 64.0\n",
            "  episodes_this_iter: 26\n",
            "  episodes_total: 1956\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.421010862427523e-21\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5920108074142086\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0033549051364427426\n",
            "          policy_loss: 0.005642980521404615\n",
            "          total_loss: 8.481618097777007\n",
            "          vf_explained_var: -0.5640326612739153\n",
            "          vf_loss: 8.475975097635741\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 280000\n",
            "    num_agent_steps_trained: 280000\n",
            "    num_steps_sampled: 280000\n",
            "    num_steps_trained: 280000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 70\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.17692307692309\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11775701049454092\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14686549256825684\n",
            "    mean_inference_ms: 1.8153471786919988\n",
            "    mean_raw_obs_processing_ms: 0.19992987652713956\n",
            "  time_since_restore: 595.2299611568451\n",
            "  time_this_iter_s: 8.451570749282837\n",
            "  time_total_s: 595.2299611568451\n",
            "  timers:\n",
            "    learn_throughput: 1042.863\n",
            "    learn_time_ms: 3835.594\n",
            "    load_throughput: 11396016.846\n",
            "    load_time_ms: 0.351\n",
            "    sample_throughput: 467.345\n",
            "    sample_time_ms: 8558.992\n",
            "    update_time_ms: 2.152\n",
            "  timestamp: 1650792093\n",
            "  timesteps_since_restore: 280000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 280000\n",
            "  training_iteration: 70\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:21:38 (running for 00:10:18.97)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">          595.23</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">  165.94</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  64</td><td style=\"text-align: right;\">            165.94</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 284000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-21-42\n",
            "  done: false\n",
            "  episode_len_mean: 160.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 160.55\n",
            "  episode_reward_min: 79.0\n",
            "  episodes_this_iter: 25\n",
            "  episodes_total: 1981\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.7105054312137616e-21\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5925544429850834\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0005795000784447138\n",
            "          policy_loss: 0.007499216374270218\n",
            "          total_loss: 8.453160793294188\n",
            "          vf_explained_var: -0.5819881795555033\n",
            "          vf_loss: 8.445661570692575\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 284000\n",
            "    num_agent_steps_trained: 284000\n",
            "    num_steps_sampled: 284000\n",
            "    num_steps_trained: 284000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 71\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.57500000000002\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.117769536624625\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1468908704471208\n",
            "    mean_inference_ms: 1.8161417771147443\n",
            "    mean_raw_obs_processing_ms: 0.19989484242043942\n",
            "  time_since_restore: 603.7167735099792\n",
            "  time_this_iter_s: 8.486812353134155\n",
            "  time_total_s: 603.7167735099792\n",
            "  timers:\n",
            "    learn_throughput: 1043.969\n",
            "    learn_time_ms: 3831.532\n",
            "    load_throughput: 11355906.322\n",
            "    load_time_ms: 0.352\n",
            "    sample_throughput: 467.14\n",
            "    sample_time_ms: 8562.736\n",
            "    update_time_ms: 2.153\n",
            "  timestamp: 1650792102\n",
            "  timesteps_since_restore: 284000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 284000\n",
            "  training_iteration: 71\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:21:47 (running for 00:10:27.58)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         603.717</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">  160.55</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  79</td><td style=\"text-align: right;\">            160.55</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 288000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-21-50\n",
            "  done: false\n",
            "  episode_len_mean: 157.4\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 157.4\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 27\n",
            "  episodes_total: 2008\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.3552527156068808e-21\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5855716326544361\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004586380095688108\n",
            "          policy_loss: 0.0021965504283466006\n",
            "          total_loss: 8.631554273123383\n",
            "          vf_explained_var: -0.4968906270560398\n",
            "          vf_loss: 8.629357720959572\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 288000\n",
            "    num_agent_steps_trained: 288000\n",
            "    num_steps_sampled: 288000\n",
            "    num_steps_trained: 288000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 72\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.55833333333332\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11776227007981306\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14686849134047766\n",
            "    mean_inference_ms: 1.8167077406528696\n",
            "    mean_raw_obs_processing_ms: 0.19989312892880462\n",
            "  time_since_restore: 612.0803036689758\n",
            "  time_this_iter_s: 8.363530158996582\n",
            "  time_total_s: 612.0803036689758\n",
            "  timers:\n",
            "    learn_throughput: 1044.539\n",
            "    learn_time_ms: 3829.44\n",
            "    load_throughput: 11416955.427\n",
            "    load_time_ms: 0.35\n",
            "    sample_throughput: 467.072\n",
            "    sample_time_ms: 8563.995\n",
            "    update_time_ms: 2.167\n",
            "  timestamp: 1650792110\n",
            "  timesteps_since_restore: 288000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 288000\n",
            "  training_iteration: 72\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:21:55 (running for 00:10:35.95)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">          612.08</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">   157.4</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">             157.4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 292000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-21-58\n",
            "  done: false\n",
            "  episode_len_mean: 158.92\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 158.92\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 2030\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 6.776263578034404e-22\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5983270578486946\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0034699327505637186\n",
            "          policy_loss: 0.005292080905568856\n",
            "          total_loss: 7.655625860152706\n",
            "          vf_explained_var: -0.5539955777506674\n",
            "          vf_loss: 7.650333742172488\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 292000\n",
            "    num_agent_steps_trained: 292000\n",
            "    num_steps_sampled: 292000\n",
            "    num_steps_trained: 292000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 73\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.05\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11773615018814824\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14686450830442582\n",
            "    mean_inference_ms: 1.8164230438835347\n",
            "    mean_raw_obs_processing_ms: 0.1998084208759151\n",
            "  time_since_restore: 620.4459984302521\n",
            "  time_this_iter_s: 8.365694761276245\n",
            "  time_total_s: 620.4459984302521\n",
            "  timers:\n",
            "    learn_throughput: 1045.975\n",
            "    learn_time_ms: 3824.182\n",
            "    load_throughput: 11478664.477\n",
            "    load_time_ms: 0.348\n",
            "    sample_throughput: 467.811\n",
            "    sample_time_ms: 8550.457\n",
            "    update_time_ms: 2.129\n",
            "  timestamp: 1650792118\n",
            "  timesteps_since_restore: 292000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 292000\n",
            "  training_iteration: 73\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:22:03 (running for 00:10:44.29)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         620.446</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  158.92</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            158.92</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 296000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-22-07\n",
            "  done: false\n",
            "  episode_len_mean: 164.09\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 164.09\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 23\n",
            "  episodes_total: 2053\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.388131789017202e-22\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5853875093562628\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0005868259853793125\n",
            "          policy_loss: 0.00737135277595371\n",
            "          total_loss: 8.16406074672617\n",
            "          vf_explained_var: -0.4261636035416716\n",
            "          vf_loss: 8.156689415952211\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 296000\n",
            "    num_agent_steps_trained: 296000\n",
            "    num_steps_sampled: 296000\n",
            "    num_steps_trained: 296000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 74\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.10000000000001\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11771825785574802\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.146766802306173\n",
            "    mean_inference_ms: 1.8159667907129378\n",
            "    mean_raw_obs_processing_ms: 0.19990205365903344\n",
            "  time_since_restore: 628.8616483211517\n",
            "  time_this_iter_s: 8.415649890899658\n",
            "  time_total_s: 628.8616483211517\n",
            "  timers:\n",
            "    learn_throughput: 1048.023\n",
            "    learn_time_ms: 3816.711\n",
            "    load_throughput: 11560129.539\n",
            "    load_time_ms: 0.346\n",
            "    sample_throughput: 467.971\n",
            "    sample_time_ms: 8547.534\n",
            "    update_time_ms: 2.045\n",
            "  timestamp: 1650792127\n",
            "  timesteps_since_restore: 296000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 296000\n",
            "  training_iteration: 74\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:22:12 (running for 00:10:52.84)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         628.862</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">  164.09</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            164.09</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 300000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-22-15\n",
            "  done: false\n",
            "  episode_len_mean: 171.42\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 171.42\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 2074\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.694065894508601e-22\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5943174884524397\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0031722666141177002\n",
            "          policy_loss: 0.002502162105614139\n",
            "          total_loss: 7.138015138205661\n",
            "          vf_explained_var: -0.3596859049412512\n",
            "          vf_loss: 7.135512985208983\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 300000\n",
            "    num_agent_steps_trained: 300000\n",
            "    num_steps_sampled: 300000\n",
            "    num_steps_trained: 300000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 75\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.60833333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11770553700955891\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14669840786743837\n",
            "    mean_inference_ms: 1.815510144159079\n",
            "    mean_raw_obs_processing_ms: 0.19994589016853553\n",
            "  time_since_restore: 637.2480270862579\n",
            "  time_this_iter_s: 8.386378765106201\n",
            "  time_total_s: 637.2480270862579\n",
            "  timers:\n",
            "    learn_throughput: 1050.036\n",
            "    learn_time_ms: 3809.392\n",
            "    load_throughput: 11504639.649\n",
            "    load_time_ms: 0.348\n",
            "    sample_throughput: 468.637\n",
            "    sample_time_ms: 8535.386\n",
            "    update_time_ms: 2.096\n",
            "  timestamp: 1650792135\n",
            "  timesteps_since_restore: 300000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 300000\n",
            "  training_iteration: 75\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:22:20 (running for 00:11:01.22)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         637.248</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  171.42</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            171.42</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 304000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-22-24\n",
            "  done: false\n",
            "  episode_len_mean: 179.05\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 179.05\n",
            "  episode_reward_min: 61.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 2095\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 8.470329472543005e-23\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5751462172436458\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0019131164407978837\n",
            "          policy_loss: 0.004337801996578452\n",
            "          total_loss: 7.125698311354524\n",
            "          vf_explained_var: -0.3222582479317983\n",
            "          vf_loss: 7.12136052500817\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 304000\n",
            "    num_agent_steps_trained: 304000\n",
            "    num_steps_sampled: 304000\n",
            "    num_steps_trained: 304000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 76\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.48333333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11770452648441651\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14656906969092864\n",
            "    mean_inference_ms: 1.8149881526401392\n",
            "    mean_raw_obs_processing_ms: 0.20009398248617535\n",
            "  time_since_restore: 645.7843022346497\n",
            "  time_this_iter_s: 8.536275148391724\n",
            "  time_total_s: 645.7843022346497\n",
            "  timers:\n",
            "    learn_throughput: 1049.647\n",
            "    learn_time_ms: 3810.806\n",
            "    load_throughput: 11348992.762\n",
            "    load_time_ms: 0.352\n",
            "    sample_throughput: 469.022\n",
            "    sample_time_ms: 8528.38\n",
            "    update_time_ms: 2.135\n",
            "  timestamp: 1650792144\n",
            "  timesteps_since_restore: 304000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 304000\n",
            "  training_iteration: 76\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:22:29 (running for 00:11:09.73)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         645.784</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">  179.05</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  61</td><td style=\"text-align: right;\">            179.05</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 308000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-22-32\n",
            "  done: false\n",
            "  episode_len_mean: 185.99\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 185.99\n",
            "  episode_reward_min: 61.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2115\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 4.2351647362715025e-23\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.590102929581878\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004596610224980103\n",
            "          policy_loss: 0.001985705181235267\n",
            "          total_loss: 6.6664554380601455\n",
            "          vf_explained_var: -0.3216712075535969\n",
            "          vf_loss: 6.66446976712955\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 308000\n",
            "    num_agent_steps_trained: 308000\n",
            "    num_steps_sampled: 308000\n",
            "    num_steps_trained: 308000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 77\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.75833333333334\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11769118677138611\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1466622440722318\n",
            "    mean_inference_ms: 1.8149898636228659\n",
            "    mean_raw_obs_processing_ms: 0.199911232984871\n",
            "  time_since_restore: 654.3191328048706\n",
            "  time_this_iter_s: 8.534830570220947\n",
            "  time_total_s: 654.3191328048706\n",
            "  timers:\n",
            "    learn_throughput: 1047.694\n",
            "    learn_time_ms: 3817.91\n",
            "    load_throughput: 11278042.485\n",
            "    load_time_ms: 0.355\n",
            "    sample_throughput: 468.879\n",
            "    sample_time_ms: 8530.983\n",
            "    update_time_ms: 2.148\n",
            "  timestamp: 1650792152\n",
            "  timesteps_since_restore: 308000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 308000\n",
            "  training_iteration: 77\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:22:38 (running for 00:11:18.39)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         654.319</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  185.99</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  61</td><td style=\"text-align: right;\">            185.99</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 312000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-22-41\n",
            "  done: false\n",
            "  episode_len_mean: 188.69\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 188.69\n",
            "  episode_reward_min: 61.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 2137\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.1175823681357513e-23\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5972803544613623\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0034130513060374574\n",
            "          policy_loss: 0.0040544798858063195\n",
            "          total_loss: 7.425169577649845\n",
            "          vf_explained_var: -0.33098021598272426\n",
            "          vf_loss: 7.421115094102839\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 312000\n",
            "    num_agent_steps_trained: 312000\n",
            "    num_steps_sampled: 312000\n",
            "    num_steps_trained: 312000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 78\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.21666666666667\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11768813341812649\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14667636243821847\n",
            "    mean_inference_ms: 1.8148569658269695\n",
            "    mean_raw_obs_processing_ms: 0.19987290523023377\n",
            "  time_since_restore: 662.7693693637848\n",
            "  time_this_iter_s: 8.450236558914185\n",
            "  time_total_s: 662.7693693637848\n",
            "  timers:\n",
            "    learn_throughput: 1048.821\n",
            "    learn_time_ms: 3813.805\n",
            "    load_throughput: 11334425.078\n",
            "    load_time_ms: 0.353\n",
            "    sample_throughput: 468.691\n",
            "    sample_time_ms: 8534.406\n",
            "    update_time_ms: 2.127\n",
            "  timestamp: 1650792161\n",
            "  timesteps_since_restore: 312000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 312000\n",
            "  training_iteration: 78\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:22:46 (running for 00:11:26.84)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         662.769</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">  188.69</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  61</td><td style=\"text-align: right;\">            188.69</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 316000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-22-49\n",
            "  done: false\n",
            "  episode_len_mean: 192.09\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.09\n",
            "  episode_reward_min: 86.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2157\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.0587911840678756e-23\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6037280794112913\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0009081130114167203\n",
            "          policy_loss: 0.007654537885419784\n",
            "          total_loss: 7.532882459958395\n",
            "          vf_explained_var: -0.4872387154127962\n",
            "          vf_loss: 7.525227913036141\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 316000\n",
            "    num_agent_steps_trained: 316000\n",
            "    num_steps_sampled: 316000\n",
            "    num_steps_trained: 316000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 79\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 77.61538461538461\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11767912087243962\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.146742813036002\n",
            "    mean_inference_ms: 1.8148786100762733\n",
            "    mean_raw_obs_processing_ms: 0.19973515835853678\n",
            "  time_since_restore: 671.3099563121796\n",
            "  time_this_iter_s: 8.540586948394775\n",
            "  time_total_s: 671.3099563121796\n",
            "  timers:\n",
            "    learn_throughput: 1046.228\n",
            "    learn_time_ms: 3823.259\n",
            "    load_throughput: 14580008.69\n",
            "    load_time_ms: 0.274\n",
            "    sample_throughput: 471.745\n",
            "    sample_time_ms: 8479.164\n",
            "    update_time_ms: 2.131\n",
            "  timestamp: 1650792169\n",
            "  timesteps_since_restore: 316000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 316000\n",
            "  training_iteration: 79\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:22:54 (running for 00:11:35.36)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          671.31</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  192.09</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  86</td><td style=\"text-align: right;\">            192.09</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 320000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-22-58\n",
            "  done: false\n",
            "  episode_len_mean: 192.51\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.51\n",
            "  episode_reward_min: 68.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 2179\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 5.293955920339378e-24\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.6002520016444627\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0007947204126714046\n",
            "          policy_loss: 0.00944104211465005\n",
            "          total_loss: 8.616502703389814\n",
            "          vf_explained_var: -0.4549045234598139\n",
            "          vf_loss: 8.607061653239752\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 320000\n",
            "    num_agent_steps_trained: 320000\n",
            "    num_steps_sampled: 320000\n",
            "    num_steps_trained: 320000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 80\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.68333333333334\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1176707508829644\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14671634623735275\n",
            "    mean_inference_ms: 1.8148253710481368\n",
            "    mean_raw_obs_processing_ms: 0.19974697205989947\n",
            "  time_since_restore: 679.8306629657745\n",
            "  time_this_iter_s: 8.52070665359497\n",
            "  time_total_s: 679.8306629657745\n",
            "  timers:\n",
            "    learn_throughput: 1046.29\n",
            "    learn_time_ms: 3823.033\n",
            "    load_throughput: 14573676.164\n",
            "    load_time_ms: 0.274\n",
            "    sample_throughput: 470.842\n",
            "    sample_time_ms: 8495.413\n",
            "    update_time_ms: 2.131\n",
            "  timestamp: 1650792178\n",
            "  timesteps_since_restore: 320000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 320000\n",
            "  training_iteration: 80\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:23:03 (running for 00:11:43.96)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         679.831</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">  192.51</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">            192.51</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 324000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-23-06\n",
            "  done: false\n",
            "  episode_len_mean: 189.53\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 189.53\n",
            "  episode_reward_min: 68.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 2201\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 2.646977960169689e-24\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5796742738575064\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0015702854452084216\n",
            "          policy_loss: -0.014219289917939453\n",
            "          total_loss: 8.203294894515826\n",
            "          vf_explained_var: -0.48173794778444434\n",
            "          vf_loss: 8.217514198569841\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 324000\n",
            "    num_agent_steps_trained: 324000\n",
            "    num_steps_sampled: 324000\n",
            "    num_steps_trained: 324000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 81\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.98333333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11765218136902486\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14672652935658614\n",
            "    mean_inference_ms: 1.8147615328698683\n",
            "    mean_raw_obs_processing_ms: 0.1997016892916796\n",
            "  time_since_restore: 688.2005660533905\n",
            "  time_this_iter_s: 8.369903087615967\n",
            "  time_total_s: 688.2005660533905\n",
            "  timers:\n",
            "    learn_throughput: 1047.56\n",
            "    learn_time_ms: 3818.398\n",
            "    load_throughput: 14639804.538\n",
            "    load_time_ms: 0.273\n",
            "    sample_throughput: 471.294\n",
            "    sample_time_ms: 8487.267\n",
            "    update_time_ms: 2.125\n",
            "  timestamp: 1650792186\n",
            "  timesteps_since_restore: 324000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 324000\n",
            "  training_iteration: 81\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:23:12 (running for 00:11:52.37)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         688.201</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  189.53</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">            189.53</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 328000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-23-15\n",
            "  done: false\n",
            "  episode_len_mean: 189.4\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 189.4\n",
            "  episode_reward_min: 68.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2221\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.3234889800848445e-24\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5904724501794384\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004407113492059118\n",
            "          policy_loss: 0.004476337483333003\n",
            "          total_loss: 8.212241318661679\n",
            "          vf_explained_var: -0.5430393975909038\n",
            "          vf_loss: 8.207764988048103\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 328000\n",
            "    num_agent_steps_trained: 328000\n",
            "    num_steps_sampled: 328000\n",
            "    num_steps_trained: 328000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 82\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.8\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11764022839946346\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14674023954980778\n",
            "    mean_inference_ms: 1.8147588700721826\n",
            "    mean_raw_obs_processing_ms: 0.199666078338943\n",
            "  time_since_restore: 696.6998856067657\n",
            "  time_this_iter_s: 8.499319553375244\n",
            "  time_total_s: 696.6998856067657\n",
            "  timers:\n",
            "    learn_throughput: 1046.842\n",
            "    learn_time_ms: 3821.017\n",
            "    load_throughput: 14443195.592\n",
            "    load_time_ms: 0.277\n",
            "    sample_throughput: 470.937\n",
            "    sample_time_ms: 8493.703\n",
            "    update_time_ms: 2.143\n",
            "  timestamp: 1650792195\n",
            "  timesteps_since_restore: 328000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 328000\n",
            "  training_iteration: 82\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:23:20 (running for 00:12:00.85)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">           696.7</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">   189.4</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">             189.4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 332000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-23-24\n",
            "  done: false\n",
            "  episode_len_mean: 190.01\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 190.01\n",
            "  episode_reward_min: 68.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2241\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 6.617444900424223e-25\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5839983639537647\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0013566747391771659\n",
            "          policy_loss: 0.003682768196668676\n",
            "          total_loss: 5.873287812356026\n",
            "          vf_explained_var: -0.17510581183177168\n",
            "          vf_loss: 5.869605040293868\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 332000\n",
            "    num_agent_steps_trained: 332000\n",
            "    num_steps_sampled: 332000\n",
            "    num_steps_trained: 332000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 83\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.10833333333333\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11763260692788666\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1467787367100214\n",
            "    mean_inference_ms: 1.8147461982046003\n",
            "    mean_raw_obs_processing_ms: 0.19957554615606818\n",
            "  time_since_restore: 705.2297720909119\n",
            "  time_this_iter_s: 8.529886484146118\n",
            "  time_total_s: 705.2297720909119\n",
            "  timers:\n",
            "    learn_throughput: 1043.593\n",
            "    learn_time_ms: 3832.913\n",
            "    load_throughput: 14580008.69\n",
            "    load_time_ms: 0.274\n",
            "    sample_throughput: 470.557\n",
            "    sample_time_ms: 8500.568\n",
            "    update_time_ms: 2.169\n",
            "  timestamp: 1650792204\n",
            "  timesteps_since_restore: 332000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 332000\n",
            "  training_iteration: 83\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:23:29 (running for 00:12:09.51)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">          705.23</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  190.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">            190.01</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 336000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-23-32\n",
            "  done: false\n",
            "  episode_len_mean: 190.73\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 190.73\n",
            "  episode_reward_min: 68.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2261\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 3.3087224502121113e-25\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5749405957037402\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0006941214280213033\n",
            "          policy_loss: 0.0014989095429579417\n",
            "          total_loss: 5.837700118813464\n",
            "          vf_explained_var: -0.05022854471719393\n",
            "          vf_loss: 5.836201187872118\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 336000\n",
            "    num_agent_steps_trained: 336000\n",
            "    num_steps_sampled: 336000\n",
            "    num_steps_trained: 336000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 84\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 78.81666666666668\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11762134571061096\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14677892267757342\n",
            "    mean_inference_ms: 1.8145829754344092\n",
            "    mean_raw_obs_processing_ms: 0.19952982978613354\n",
            "  time_since_restore: 713.602205991745\n",
            "  time_this_iter_s: 8.37243390083313\n",
            "  time_total_s: 713.602205991745\n",
            "  timers:\n",
            "    learn_throughput: 1042.373\n",
            "    learn_time_ms: 3837.398\n",
            "    load_throughput: 14417131.563\n",
            "    load_time_ms: 0.277\n",
            "    sample_throughput: 470.383\n",
            "    sample_time_ms: 8503.702\n",
            "    update_time_ms: 2.303\n",
            "  timestamp: 1650792212\n",
            "  timesteps_since_restore: 336000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 336000\n",
            "  training_iteration: 84\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:23:37 (running for 00:12:17.87)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         713.602</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">  190.73</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">            190.73</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 340000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-23-40\n",
            "  done: false\n",
            "  episode_len_mean: 193.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 193.55\n",
            "  episode_reward_min: 111.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 2282\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 1.6543612251060557e-25\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5836896336847736\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00019154569277586418\n",
            "          policy_loss: -0.005443457808465727\n",
            "          total_loss: 5.535590509958165\n",
            "          vf_explained_var: -0.11281915326272288\n",
            "          vf_loss: 5.541033950800537\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 340000\n",
            "    num_agent_steps_trained: 340000\n",
            "    num_steps_sampled: 340000\n",
            "    num_steps_trained: 340000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 85\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.12500000000001\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11760992702397974\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14677365898362157\n",
            "    mean_inference_ms: 1.8142731785562571\n",
            "    mean_raw_obs_processing_ms: 0.1994740286633255\n",
            "  time_since_restore: 721.9244272708893\n",
            "  time_this_iter_s: 8.322221279144287\n",
            "  time_total_s: 721.9244272708893\n",
            "  timers:\n",
            "    learn_throughput: 1042.67\n",
            "    learn_time_ms: 3836.303\n",
            "    load_throughput: 12424806.339\n",
            "    load_time_ms: 0.322\n",
            "    sample_throughput: 470.452\n",
            "    sample_time_ms: 8502.467\n",
            "    update_time_ms: 2.278\n",
            "  timestamp: 1650792220\n",
            "  timesteps_since_restore: 340000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 340000\n",
            "  training_iteration: 85\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:23:45 (running for 00:12:26.17)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>RUNNING </td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         721.924</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  193.55</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 111</td><td style=\"text-align: right;\">            193.55</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_80d83_00000:\n",
            "  agent_timesteps_total: 344000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-04-24_09-23-49\n",
            "  done: true\n",
            "  episode_len_mean: 196.11\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 196.11\n",
            "  episode_reward_min: 111.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2302\n",
            "  experiment_id: 495e8b46f96c44a5bba45679f3ae141f\n",
            "  hostname: 4671fbd62325\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          allreduce_latency: 0.0\n",
            "          cur_kl_coeff: 8.271806125530278e-26\n",
            "          cur_lr: 5.0000000000000016e-05\n",
            "          entropy: 0.5750028940939134\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0005383986887342405\n",
            "          policy_loss: 0.004769039234166504\n",
            "          total_loss: 6.518303403803097\n",
            "          vf_explained_var: -0.1406329942005937\n",
            "          vf_loss: 6.513534375929063\n",
            "        model: {}\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 344000\n",
            "    num_agent_steps_trained: 344000\n",
            "    num_steps_sampled: 344000\n",
            "    num_steps_trained: 344000\n",
            "    num_steps_trained_this_iter: 4000\n",
            "  iterations_since_restore: 86\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 79.3\n",
            "    ram_util_percent: 26.0\n",
            "  pid: 3156\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.11759808507502796\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.14679787778803177\n",
            "    mean_inference_ms: 1.8140772305184436\n",
            "    mean_raw_obs_processing_ms: 0.19937241000721811\n",
            "  time_since_restore: 730.4131088256836\n",
            "  time_this_iter_s: 8.488681554794312\n",
            "  time_total_s: 730.4131088256836\n",
            "  timers:\n",
            "    learn_throughput: 1042.607\n",
            "    learn_time_ms: 3836.535\n",
            "    load_throughput: 12362549.554\n",
            "    load_time_ms: 0.324\n",
            "    sample_throughput: 470.783\n",
            "    sample_time_ms: 8496.482\n",
            "    update_time_ms: 2.227\n",
            "  timestamp: 1650792229\n",
            "  timesteps_since_restore: 344000\n",
            "  timesteps_this_iter: 4000\n",
            "  timesteps_total: 344000\n",
            "  training_iteration: 86\n",
            "  trial_id: 80d83_00000\n",
            "  warmup_time: 10.175345420837402\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-04-24 09:23:49 (running for 00:12:29.73)<br>Memory usage on this node: 3.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/3 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_80d83_00000</td><td>TERMINATED</td><td>172.28.0.2:3156</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         730.413</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">  196.11</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 111</td><td style=\"text-align: right;\">            196.11</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-24 09:23:49,677\tINFO tune.py:702 -- Total run time: 750.20 seconds (749.72 seconds for the tuning loop).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parameter_search_config = {\n",
        "    \"env\": \"CartPole-v0\",\n",
        "    \"framework\": \"torch\",\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    \"model\": {\n",
        "      \"fcnet_hiddens\": ray.tune.grid_search([[32], [64]]),\n",
        "      \"fcnet_activation\": ray.tune.grid_search([\"linear\", \"relu\"]),\n",
        "      \"dueling\": ray.tune.grid_search([True, False]),\n",
        "      \"double_q\": ray.tune.grid_search([True, False])\n",
        "    },\n",
        "    \"lr\": ray.tune.uniform(1e-7, 1e-2)\n",
        "}\n",
        "\n",
        "# To explicitly stop or restart Ray, use the shutdown API.\n",
        "ray.shutdown()\n",
        "\n",
        "ray.init(\n",
        "  num_cpus=12,\n",
        "  include_dashboard=False,\n",
        "  ignore_reinit_error=True,\n",
        "  log_to_driver=False,\n",
        ")\n",
        "\n",
        "parameter_search_analysis = ray.tune.run(\n",
        "  \"PPO\",\n",
        "  config=parameter_search_config,\n",
        "  stop=stop,\n",
        "  num_samples=5,\n",
        "  metric=\"timesteps_total\",\n",
        "  mode=\"min\",\n",
        ")\n",
        "\n",
        "print(\n",
        "  \"Best hyperparameters found:\",\n",
        "  parameter_search_analysis.best_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ba0B5SIeDLG",
        "outputId": "5924f377-df63-4466-b723-245c0463d872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-24 09:23:56,238\tINFO trial_runner.py:803 -- starting PPO_CartPole-v0_43c3f_00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZZCUqhlcQaNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model building\n",
        " "
      ],
      "metadata": {
        "id": "sQ_lpADbQpaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import pyvirtualdisplay\n",
        "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "except ImportError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "FYb78QZ1Qtnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "id": "CrdKb96cQul8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_environment(env, figsize=(5,4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    return img"
      ],
      "metadata": {
        "id": "A7DA_NhEQzCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pVPd7hgyQ4WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_memory.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info"
      ],
      "metadata": {
        "id": "t0__sVxFR0Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_policy_net(model, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "        left_proba = model.predict(obs.reshape(1, -1))\n",
        "        action = int(np.random.rand() > left_proba)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            break\n",
        "    env.close()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "_jxTTCIpQ72d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])"
      ],
      "metadata": {
        "id": "YRoidjAlRFTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "K = keras.backend\n",
        "input_states = keras.layers.Input(shape=[4])\n",
        "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "state_values = keras.layers.Dense(1)(hidden2)\n",
        "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
        "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
        "Q_values = state_values + advantages\n",
        "model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
        "\n",
        "target = keras.models.clone_model(model)\n",
        "target.set_weights(model.get_weights())"
      ],
      "metadata": {
        "id": "tc4SlY53RL4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(learning_rate=7.5e-3)\n",
        "loss_fn = keras.losses.Huber()\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
        "    target_Q_values = (rewards + \n",
        "                       (1 - dones) * discount_rate * next_best_Q_values)\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "AH887WL7RWap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_memory = deque(maxlen=2000)"
      ],
      "metadata": {
        "id": "8uEUWWNKRbQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "rewards = []\n",
        "best_score = 0\n",
        "\n",
        "for episode in range(600):\n",
        "    obs = env.reset()    \n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "        if done:\n",
        "            break\n",
        "    rewards.append(step)\n",
        "    if step >= best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n",
        "    if episode >= 50:\n",
        "        training_step(batch_size)\n",
        "        if episode % 50 == 0:\n",
        "            target.set_weights(model.get_weights())\n",
        "\n",
        "model.set_weights(best_weights)"
      ],
      "metadata": {
        "id": "f8mN0dylRfQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Sum of rewards\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qYPqq97rRjD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fOcImIeWRo75"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}